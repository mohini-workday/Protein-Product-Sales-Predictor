{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "405fe215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting shap\n",
      "  Using cached shap-0.50.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy>=2 in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from shap) (2.2.4)\n",
      "Requirement already satisfied: scipy in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from shap) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from shap) (1.6.1)\n",
      "Requirement already satisfied: pandas in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from shap) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from shap) (24.2)\n",
      "Requirement already satisfied: slicer==0.0.8 in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from shap) (0.62.1)\n",
      "Requirement already satisfied: cloudpickle in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from shap) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from shap) (4.15.0)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from numba>=0.54->shap) (0.45.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from scikit-learn->shap) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
      "Using cached shap-0.50.0-cp312-cp312-macosx_11_0_arm64.whl (555 kB)\n",
      "Installing collected packages: shap\n",
      "Successfully installed shap-0.50.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Product Label → Sales: Clean, modular ML pipeline\n",
    "# Paste into a new notebook cell or save as pipeline.py\n",
    "# =========================\n",
    "\n",
    "# 0. Requirements (install if needed)\n",
    "# !pip install pandas numpy scikit-learn shap matplotlib seaborn xgboost category_encoders joblib\n",
    "\n",
    "# 1. Imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "\n",
    "# Optional XGBoost\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    has_xgb = True\n",
    "except Exception:\n",
    "    has_xgb = False\n",
    "\n",
    "# SHAP\n",
    "%pip install shap\n",
    "import shap\n",
    "\n",
    "# utilities\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa049cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: shap in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (0.50.0)\n",
      "Requirement already satisfied: numpy>=2 in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from shap) (2.2.4)\n",
      "Requirement already satisfied: scipy in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from shap) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from shap) (1.6.1)\n",
      "Requirement already satisfied: pandas in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from shap) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from shap) (24.2)\n",
      "Requirement already satisfied: slicer==0.0.8 in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from shap) (0.62.1)\n",
      "Requirement already satisfied: cloudpickle in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from shap) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from shap) (4.15.0)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from numba>=0.54->shap) (0.45.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from scikit-learn->shap) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "\n",
    "# Optional XGBoost\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    has_xgb = True\n",
    "except Exception:\n",
    "    has_xgb = False\n",
    "\n",
    "# SHAP\n",
    "%pip install shap\n",
    "import shap\n",
    "\n",
    "# utilities\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "182d8aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2. Utility / helper functions\n",
    "# =========================\n",
    "\n",
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load dataset - adjust if your file format differs.\"\"\"\n",
    "    if path.endswith('.csv'):\n",
    "        return pd.read_csv(path)\n",
    "    elif path.endswith('.xlsx') or path.endswith('.xls'):\n",
    "        return pd.read_excel(path)\n",
    "    elif path.endswith('.parquet'):\n",
    "        return pd.read_parquet(path)\n",
    "    elif path.endswith('.ipynb'):\n",
    "        raise ValueError(\"Notebook input given — load the contained CSV/Excel instead.\")\n",
    "    else:\n",
    "        return pd.read_csv(path, low_memory=False)\n",
    "\n",
    "def summarize_df(df: pd.DataFrame, n: int = 5):\n",
    "    print(\"Shape:\", df.shape)\n",
    "    display(df.head(n))\n",
    "    display(df.describe(include='all').T)\n",
    "\n",
    "def identify_feature_groups(df: pd.DataFrame,\n",
    "                            text_features: Optional[List[str]] = None,\n",
    "                            numeric_features: Optional[List[str]] = None,\n",
    "                            categorical_features: Optional[List[str]] = None) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Identify candidate features automatically if the user didn't provide them.\n",
    "    This is conservative: uses dtype heuristics.\n",
    "    \"\"\"\n",
    "    if text_features is None:\n",
    "        text_features = [c for c in df.columns if df[c].dtype == \"object\" and df[c].nunique() > 20][:2]  # first 2 long texts\n",
    "    if numeric_features is None:\n",
    "        numeric_features = [c for c in df.select_dtypes(include=[np.number]).columns if c != 'Sales']  # exclude target name 'Sales' if present\n",
    "    if categorical_features is None:\n",
    "        categorical_features = [c for c in df.select_dtypes(include=['object', 'category']).columns if c not in text_features]\n",
    "    return {\"text\": text_features, \"num\": numeric_features, \"cat\": categorical_features}\n",
    "\n",
    "# Small helper to create label-specific engineered features — customize these based on your dataset\n",
    "def create_label_features(df: pd.DataFrame, label_column: str = \"LabelText\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Example label-derived features:\n",
    "     - label_text_length\n",
    "     - label_word_count\n",
    "     - label_has_numeric (e.g., nutritional numbers)\n",
    "     - label_logo_present (if you have a boolean column or detect via presence of 'logo' in metadata)\n",
    "    You should replace heuristics with actual detection if you have label images or annotations.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if label_column in df.columns:\n",
    "        df['label_text_length'] = df[label_column].fillna('').apply(len)\n",
    "        df['label_word_count'] = df[label_column].fillna('').apply(lambda s: len(s.split()))\n",
    "        df['label_has_digits'] = df[label_column].fillna('').apply(lambda s: any(ch.isdigit() for ch in s)).astype(int)\n",
    "    else:\n",
    "        # If no label text column, try heuristics on other text fields or add placeholders\n",
    "        df['label_text_length'] = 0\n",
    "        df['label_word_count'] = 0\n",
    "        df['label_has_digits'] = 0\n",
    "    return df\n",
    "\n",
    "# Simple evaluation function\n",
    "def evaluate_regression(y_true, y_pred) -> Dict[str, float]:\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {\"RMSE\": rmse, \"MAE\": mae, \"R2\": r2}\n",
    "\n",
    "# Plotting helper for feature importances\n",
    "def plot_feature_importances(features: List[str], importances: np.ndarray, top_n: int = 25, title: str = \"Feature importances\"):\n",
    "    fi = pd.DataFrame({\"feature\": features, \"importance\": importances})\n",
    "    fi = fi.sort_values(\"importance\", ascending=False).head(top_n)\n",
    "    plt.figure(figsize=(8, min(6, 0.25*len(fi))))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=fi)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17c66fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3. Preprocessing + Pipelines\n",
    "# =========================\n",
    "\n",
    "def build_preprocessor(numeric_features: List[str],\n",
    "                       categorical_features: List[str],\n",
    "                       text_features: List[str]) -> ColumnTransformer:\n",
    "    \"\"\"\n",
    "    Build ColumnTransformer for numeric, categorical, and text features.\n",
    "    Text features are vectorized with TF-IDF and then truncated (sparse).\n",
    "    \"\"\"\n",
    "    # numeric pipeline\n",
    "    num_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "    # categorical pipeline\n",
    "    cat_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False)),\n",
    "    ])\n",
    "\n",
    "    # text pipeline: combine each text field's TFIDF into a single sparse matrix via FeatureUnion if needed.\n",
    "    # We'll handle only the first text field for simplicity with TF-IDF. Extendable.\n",
    "    text_transformers = []\n",
    "    if text_features:\n",
    "        # Use Tfidf for the first text column; if you have more, create separate transformers and combine.\n",
    "        tfidf = Pipeline([\n",
    "            ('imputer', FunctionTransformer(lambda X: X.fillna('').astype(str), validate=False)),\n",
    "            ('tfidf', TfidfVectorizer(max_features=2000, ngram_range=(1,2)))\n",
    "        ])\n",
    "        # We'll use ColumnTransformer's 'remainder' to keep things simple by using a custom function below.\n",
    "        # But ColumnTransformer cannot directly accept TfidfVectorizer for column names in older sklearn; so we'll handle in apply_text_features() step.\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', num_pipe, numeric_features),\n",
    "        ('cat', cat_pipe, categorical_features),\n",
    "        # text will be handled separately to give more control\n",
    "    ], remainder='drop', sparse_threshold=0)\n",
    "    return preprocessor\n",
    "\n",
    "# Text vectorizer wrapper to integrate with preprocessor output\n",
    "from scipy.sparse import hstack\n",
    "def transform_with_text(preprocessor: ColumnTransformer, df: pd.DataFrame, text_features: List[str], tfidf_vectorizer: Optional[TfidfVectorizer] = None):\n",
    "    \"\"\"Return (X, feature_names) combining preprocessor output (dense) and TF-IDF (sparse) for text.\"\"\"\n",
    "    X_pre = preprocessor.fit_transform(df)\n",
    "    # feature names for numeric + categorical (OneHot)\n",
    "    # Construct feature names carefully:\n",
    "    feature_names = []\n",
    "    # numeric names\n",
    "    for t in preprocessor.transformers_:\n",
    "        name, transformer, cols = t\n",
    "        if name == 'num':\n",
    "            feature_names.extend(list(cols))\n",
    "        elif name == 'cat':\n",
    "            # get feature names from encoder\n",
    "            ohe = transformer.named_steps['onehot']\n",
    "            # get categories if available\n",
    "            ohe_features = []\n",
    "            if hasattr(ohe, 'get_feature_names_out'):\n",
    "                ohe_features = list(ohe.get_feature_names_out(cols))\n",
    "            else:\n",
    "                # fallback: use column names\n",
    "                ohe_features = list(cols)\n",
    "            feature_names.extend(ohe_features)\n",
    "    # text\n",
    "    tfidf = tfidf_vectorizer or TfidfVectorizer(max_features=2000, ngram_range=(1,2))\n",
    "    if text_features:\n",
    "        # combine the text columns into a single string per row\n",
    "        text_series = df[text_features].fillna('').astype(str).agg(' '.join, axis=1)\n",
    "        X_text = tfidf.fit_transform(text_series)\n",
    "        # append text feature names\n",
    "        tf_names = [\"tfidf_\" + t for t in tfidf.get_feature_names_out()]\n",
    "        feature_names.extend(tf_names)\n",
    "        # combine dense and sparse\n",
    "        if hasattr(X_pre, \"toarray\"): X_pre = np.array(X_pre)\n",
    "        try:\n",
    "            from scipy import sparse\n",
    "            if sparse.issparse(X_pre):\n",
    "                X_full = hstack([X_pre, X_text])\n",
    "            else:\n",
    "                X_full = hstack([sparse.csr_matrix(X_pre), X_text])\n",
    "        except Exception:\n",
    "            X_full = np.hstack([X_pre.toarray() if hasattr(X_pre, \"toarray\") else X_pre, X_text.toarray()])\n",
    "        return X_full, feature_names, tfidf\n",
    "    else:\n",
    "        # no text\n",
    "        return X_pre, feature_names, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e63b73e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4. Modeling utilities\n",
    "# =========================\n",
    "\n",
    "def get_models(random_state: int = 42):\n",
    "    models = {\n",
    "        \"Linear\": LinearRegression(),\n",
    "        \"Ridge\": Ridge(random_state=random_state),\n",
    "        \"Lasso\": Lasso(random_state=random_state),\n",
    "        \"RandomForest\": RandomForestRegressor(n_estimators=200, random_state=random_state, n_jobs=-1),\n",
    "        \"GBDT\": GradientBoostingRegressor(n_estimators=200, random_state=random_state),\n",
    "    }\n",
    "    if has_xgb:\n",
    "        models[\"XGBoost\"] = XGBRegressor(n_estimators=200, random_state=random_state, n_jobs=-1, objective='reg:squarederror')\n",
    "    return models\n",
    "\n",
    "def cross_validate_models(models: Dict[str, object], X, y, cv=5, scoring='r2'):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"Cross-validating {name} ...\")\n",
    "        scores = cross_val_score(model, X, y, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "        results[name] = {\"mean_score\": float(np.mean(scores)), \"std\": float(np.std(scores)), \"all_scores\": scores}\n",
    "        print(f\"  {name}: mean {scoring} = {results[name]['mean_score']:.4f} ± {results[name]['std']:.4f}\")\n",
    "    return results\n",
    "\n",
    "# Train final model and evaluate\n",
    "def train_and_evaluate(model, X_train, y_train, X_valid, y_valid, feature_names: Optional[List[str]] = None):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_valid = model.predict(X_valid)\n",
    "    metrics_train = evaluate_regression(y_train, preds_train)\n",
    "    metrics_valid = evaluate_regression(y_valid, preds_valid)\n",
    "    print(\"Train:\", metrics_train)\n",
    "    print(\"Valid:\", metrics_valid)\n",
    "    if feature_names is not None and hasattr(model, \"feature_importances_\"):\n",
    "        plot_feature_importances(feature_names, model.feature_importances_, top_n=30, title=f\"{model.__class__.__name__} feature importances\")\n",
    "    return model, metrics_train, metrics_valid\n",
    "\n",
    "# Permutation importance wrapper\n",
    "def compute_permutation_importance(model, X_valid, y_valid, feature_names, n_repeats=10):\n",
    "    r = permutation_importance(model, X_valid, y_valid, n_repeats=n_repeats, random_state=0, n_jobs=-1)\n",
    "    sorted_idx = r.importances_mean.argsort()[::-1]\n",
    "    importances = r.importances_mean[sorted_idx]\n",
    "    names = [feature_names[i] for i in sorted_idx]\n",
    "    plot_feature_importances(names, importances, top_n=30, title=\"Permutation importances (validation)\")\n",
    "    return r, names, importances\n",
    "\n",
    "# SHAP analysis\n",
    "def shap_analysis(model, X_sample, feature_names, model_name=\"model\", n_samples=1000):\n",
    "    \"\"\"\n",
    "    For tree-based models use TreeExplainer. For linear models KernelExplainer can be slow.\n",
    "    X_sample can be a numpy array or DataFrame.\n",
    "    \"\"\"\n",
    "    # convert to DataFrame for shap labels if possible\n",
    "    if hasattr(X_sample, \"toarray\"):\n",
    "        # might be sparse\n",
    "        try:\n",
    "            X_df = pd.DataFrame(X_sample.toarray(), columns=feature_names)\n",
    "        except Exception:\n",
    "            X_df = pd.DataFrame(X_sample.todense(), columns=feature_names)\n",
    "    else:\n",
    "        X_df = pd.DataFrame(X_sample, columns=feature_names)\n",
    "\n",
    "    if model.__class__.__name__ in [\"RandomForestRegressor\", \"GradientBoostingRegressor\", \"XGBRegressor\"]:\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_df)\n",
    "    else:\n",
    "        # fallback\n",
    "        explainer = shap.Explainer(model.predict, X_df)\n",
    "        shap_values = explainer(X_df)\n",
    "    # summary plot\n",
    "    shap.summary_plot(shap_values, X_df, show=True)\n",
    "    # dependence plot for top label-related features if they exist\n",
    "    return explainer, shap_values\n",
    "\n",
    "# Label-sensitivity scoring\n",
    "def label_sensitivity_score(shap_values, feature_names, label_feature_patterns: List[str]):\n",
    "    \"\"\"\n",
    "    Compute a simple label-sensitivity score: sum of absolute SHAP values for features matching label patterns,\n",
    "    relative to total absolute SHAP values.\n",
    "    \"\"\"\n",
    "    # shap_values: if shap returns array-like per sample\n",
    "    if hasattr(shap_values, 'values'):\n",
    "        sv = shap_values.values\n",
    "    else:\n",
    "        sv = shap_values\n",
    "    abs_sv = np.abs(sv)\n",
    "    total_by_feat = abs_sv.mean(axis=0)\n",
    "    feature_df = pd.DataFrame({\"feature\": feature_names, \"mean_abs_shap\": total_by_feat})\n",
    "    # sum features matching patterns\n",
    "    mask = feature_df['feature'].str.contains('|'.join(label_feature_patterns), regex=True)\n",
    "    label_importance = feature_df.loc[mask, 'mean_abs_shap'].sum()\n",
    "    total = feature_df['mean_abs_shap'].sum()\n",
    "    score = float(label_importance / total) if total > 0 else 0.0\n",
    "    return {\"label_importance\": label_importance, \"total_importance\": total, \"score\": score, \"matching_features\": feature_df.loc[mask].sort_values('mean_abs_shap', ascending=False)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "604bde25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 5. Example end-to-end usage\n",
    "# =========================\n",
    "\n",
    "def run_full_workflow(df: pd.DataFrame,\n",
    "                      target: str = \"Sales\",\n",
    "                      test_size: float = 0.2,\n",
    "                      random_state: int = 42,\n",
    "                      text_columns: Optional[List[str]] = None,\n",
    "                      label_text_column: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Full run: feature engineering, preprocessor build, model CV, final training, SHAP, permutation importance.\n",
    "    \"\"\"\n",
    "    # 1) Quick summary\n",
    "    summarize_df(df, n=3)\n",
    "\n",
    "    # 2) Basic feature engineering for labels\n",
    "    if label_text_column is None and 'LabelText' in df.columns:\n",
    "        label_text_column = 'LabelText'\n",
    "    if label_text_column:\n",
    "        df = create_label_features(df, label_column=label_text_column)\n",
    "    # 3) Identify features\n",
    "    groups = identify_feature_groups(df, text_features=text_columns)\n",
    "    # ensure target present\n",
    "    if target not in df.columns:\n",
    "        raise ValueError(f\"Target '{target}' not found in dataframe columns.\")\n",
    "\n",
    "    # Drop rows with missing target\n",
    "    df = df[df[target].notna()].copy()\n",
    "\n",
    "    # Select features: all numeric + categorical + text-derived label features\n",
    "    numeric_features = groups['num']\n",
    "    categorical_features = groups['cat']\n",
    "    text_features = groups['text'] if text_columns is None else text_columns\n",
    "\n",
    "    # Ensure label-derived numeric features are in numeric_features\n",
    "    potential_label_feats = [c for c in df.columns if c.startswith('label_')]\n",
    "    for lf in potential_label_feats:\n",
    "        if lf not in numeric_features:\n",
    "            numeric_features.append(lf)\n",
    "\n",
    "    # 4) train/valid split\n",
    "    X = df[numeric_features + categorical_features + (text_features if text_features else [])]\n",
    "    y = df[target]\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # 5) Build preprocessor\n",
    "    preprocessor = build_preprocessor(numeric_features, categorical_features, text_features)\n",
    "    X_train_trans, feature_names, tfidf = transform_with_text(preprocessor, X_train, text_features)\n",
    "    X_valid_trans, _, _ = transform_with_text(preprocessor, X_valid, text_features, tfidf_vectorizer=tfidf)  # reuse same vectorizer\n",
    "\n",
    "    # 6) Compare models via CV on training set (use a lightweight approach: 3-fold cv)\n",
    "    models = get_models(random_state=random_state)\n",
    "    cv_results = cross_validate_models(models, X_train_trans, y_train, cv=3, scoring='r2')\n",
    "\n",
    "    # 7) Choose best model (by mean CV r2)\n",
    "    best_name = max(cv_results.items(), key=lambda kv: kv[1]['mean_score'])[0]\n",
    "    print(\"Best by CV (r2):\", best_name)\n",
    "    best_model = models[best_name]\n",
    "\n",
    "    # 8) Train final model and evaluate\n",
    "    trained_model, metrics_train, metrics_valid = train_and_evaluate(best_model, X_train_trans, y_train, X_valid_trans, y_valid, feature_names)\n",
    "\n",
    "    # 9) Permutation importance on validation\n",
    "    try:\n",
    "        perm_res = compute_permutation_importance(trained_model, X_valid_trans, y_valid, feature_names)\n",
    "    except Exception as e:\n",
    "        print(\"Permutation importance failed:\", e)\n",
    "        perm_res = None\n",
    "\n",
    "    # 10) SHAP analysis\n",
    "    try:\n",
    "        # sample some rows to speed up shap\n",
    "        n_shap = min(500, X_valid_trans.shape[0])\n",
    "        X_shap_sample = X_valid_trans[:n_shap]\n",
    "        explainer, shap_values = shap_analysis(trained_model, X_shap_sample, feature_names, model_name=best_name)\n",
    "        # label sensitivity\n",
    "        label_patterns = ['label_', 'tfidf_', 'Color', 'color', 'logo', 'Logo']  # heuristics: extend based on your dataset\n",
    "        sens = label_sensitivity_score(shap_values, feature_names, label_patterns)\n",
    "        print(\"Label sensitivity score:\", sens['score'])\n",
    "        display(sens['matching_features'].head(20))\n",
    "    except Exception as e:\n",
    "        print(\"SHAP analysis failed:\", e)\n",
    "        shap_values = None\n",
    "        sens = None\n",
    "\n",
    "    # 11) Return objects for further analysis / saving\n",
    "    return {\n",
    "        \"trained_model\": trained_model,\n",
    "        \"feature_names\": feature_names,\n",
    "        \"preprocessor\": preprocessor,\n",
    "        \"tfidf_vectorizer\": tfidf,\n",
    "        \"X_train_trans\": X_train_trans,\n",
    "        \"X_valid_trans\": X_valid_trans,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_valid\": y_valid,\n",
    "        \"cv_results\": cv_results,\n",
    "        \"perm_res\": perm_res,\n",
    "        \"shap\": {\"explainer\": explainer if 'explainer' in locals() else None, \"shap_values\": shap_values if 'shap_values' in locals() else None, \"sensitivity\": sens}\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
