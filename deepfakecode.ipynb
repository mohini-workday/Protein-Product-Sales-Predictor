{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c730ba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DEEP FAKE VIDEO DETECTION - ANN THESIS PROJECT\n",
    "# CRISP-DM Methodology Implementation\n",
    "# Dataset: Deep Fake Detection (DFD) Dataset from HuggingFace\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "PROJECT STRUCTURE:\n",
    "1. Business Understanding\n",
    "2. Data Understanding\n",
    "3. Data Preparation\n",
    "4. Modeling (Multiple Approaches)\n",
    "5. Evaluation\n",
    "6. Deployment/Results\n",
    "\n",
    "Author: Thesis Student\n",
    "Date: 2025\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 0: ENVIRONMENT SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q datasets huggingface_hub\n",
    "!pip install -q opencv-python-headless\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q matplotlib seaborn\n",
    "!pip install -q pillow\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q timm  # PyTorch Image Models\n",
    "!pip install -q grad-cam  # For explainability\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "import timm\n",
    "\n",
    "# Computer Vision\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# ML & Evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report,\n",
    "                             roc_curve, auc, roc_auc_score)\n",
    "\n",
    "# HuggingFace\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Utilities\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: BUSINESS UNDERSTANDING\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "PROJECT OBJECTIVE:\n",
    "Develop an Artificial Neural Network model to detect deepfake videos with:\n",
    "- High accuracy (>90%)\n",
    "- Good generalization\n",
    "- Interpretability/Explainability\n",
    "- Comparison of multiple architectures\n",
    "\n",
    "BUSINESS VALUE:\n",
    "- Combat misinformation\n",
    "- Protect digital identity\n",
    "- Ensure media authenticity\n",
    "\n",
    "SUCCESS CRITERIA:\n",
    "- Accuracy > 90%\n",
    "- F1-Score > 0.88\n",
    "- Low false negative rate (don't miss deepfakes)\n",
    "- Model explainability through visualization\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DEEP FAKE DETECTION PROJECT - BUSINESS UNDERSTANDING\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nObjective: Detect deepfake videos using deep learning\")\n",
    "print(\"Target Metric: Accuracy > 90%, F1-Score > 0.88\")\n",
    "print(\"Approach: Compare multiple CNN architectures + Transfer Learning\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: DATA UNDERSTANDING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: DATA UNDERSTANDING - LOADING DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load dataset from HuggingFace\n",
    "print(\"\\n[INFO] Loading dataset from HuggingFace...\")\n",
    "print(\"This may take several minutes depending on dataset size...\")\n",
    "\n",
    "try:\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(\"Hemgg/deep-fake-detection-dfd-entire-original-dataset\")\n",
    "    \n",
    "    print(\"\\n✓ Dataset loaded successfully!\")\n",
    "    print(f\"\\nDataset structure: {dataset}\")\n",
    "    \n",
    "    # Explore the dataset\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"DATASET EXPLORATION\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Check available splits\n",
    "    print(f\"\\nAvailable splits: {list(dataset.keys())}\")\n",
    "    \n",
    "    # Get the training data\n",
    "    train_data = dataset['train']\n",
    "    \n",
    "    print(f\"\\nNumber of samples in train split: {len(train_data)}\")\n",
    "    print(f\"\\nColumn names: {train_data.column_names}\")\n",
    "    print(f\"\\nFeatures: {train_data.features}\")\n",
    "    \n",
    "    # Show first sample\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"SAMPLE DATA INSPECTION\")\n",
    "    print(\"-\"*80)\n",
    "    sample = train_data[0]\n",
    "    print(\"\\nFirst sample keys:\", sample.keys())\n",
    "    for key, value in sample.items():\n",
    "        if key == 'video':\n",
    "            print(f\"\\n{key}: <Video data - shape/type: {type(value)}>\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Error loading dataset: {e}\")\n",
    "    print(\"\\nTroubleshooting tips:\")\n",
    "    print(\"1. Check internet connection\")\n",
    "    print(\"2. Verify HuggingFace dataset URL\")\n",
    "    print(\"3. Try: !huggingface-cli login\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXPLORATORY DATA ANALYSIS (EDA)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPLORATORY DATA ANALYSIS (EDA)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Function to analyze dataset distribution\n",
    "def analyze_dataset(dataset, split_name='train'):\n",
    "    \"\"\"Perform comprehensive EDA on the dataset\"\"\"\n",
    "    \n",
    "    print(f\"\\n[INFO] Analyzing {split_name} split...\")\n",
    "    \n",
    "    # Convert to pandas for easier analysis\n",
    "    df = pd.DataFrame(dataset)\n",
    "    \n",
    "    print(\"\\n1. BASIC STATISTICS\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "    \n",
    "    # Check for label column\n",
    "    label_columns = [col for col in df.columns if 'label' in col.lower()]\n",
    "    \n",
    "    if label_columns:\n",
    "        label_col = label_columns[0]\n",
    "        print(f\"\\n2. LABEL DISTRIBUTION (Column: {label_col})\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        label_counts = df[label_col].value_counts()\n",
    "        print(f\"\\n{label_counts}\")\n",
    "        \n",
    "        # Calculate percentages\n",
    "        label_percentages = df[label_col].value_counts(normalize=True) * 100\n",
    "        print(f\"\\nPercentages:\\n{label_percentages}\")\n",
    "        \n",
    "        # Visualize distribution\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Bar plot\n",
    "        label_counts.plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
    "        axes[0].set_title('Label Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlabel('Label', fontsize=12)\n",
    "        axes[0].set_ylabel('Count', fontsize=12)\n",
    "        axes[0].tick_params(axis='x', rotation=0)\n",
    "        \n",
    "        # Pie chart\n",
    "        colors = ['#2ecc71', '#e74c3c']\n",
    "        axes[1].pie(label_counts, labels=label_counts.index, autopct='%1.1f%%',\n",
    "                   colors=colors, startangle=90)\n",
    "        axes[1].set_title('Label Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('label_distribution.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Check for class imbalance\n",
    "        imbalance_ratio = label_counts.max() / label_counts.min()\n",
    "        print(f\"\\n3. CLASS IMBALANCE ANALYSIS\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"Imbalance Ratio: {imbalance_ratio:.2f}\")\n",
    "        if imbalance_ratio > 1.5:\n",
    "            print(\"⚠ Warning: Significant class imbalance detected!\")\n",
    "            print(\"  → Consider using weighted loss or oversampling\")\n",
    "        else:\n",
    "            print(\"✓ Classes are relatively balanced\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(f\"\\n4. MISSING VALUES\")\n",
    "    print(\"-\"*80)\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(missing[missing > 0])\n",
    "    else:\n",
    "        print(\"✓ No missing values found\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run EDA\n",
    "try:\n",
    "    df_analysis = analyze_dataset(train_data, 'train')\n",
    "except Exception as e:\n",
    "    print(f\"Error during EDA: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: DATA PREPARATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: DATA PREPARATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Custom Dataset Class for Video/Image Data\n",
    "class DeepfakeDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for Deepfake Detection\"\"\"\n",
    "    \n",
    "    def __init__(self, hf_dataset, transform=None, max_samples=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hf_dataset: HuggingFace dataset\n",
    "            transform: torchvision transforms\n",
    "            max_samples: Limit number of samples (for testing)\n",
    "        \"\"\"\n",
    "        self.dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "        \n",
    "        if max_samples:\n",
    "            self.dataset = self.dataset.select(range(min(max_samples, len(self.dataset))))\n",
    "        \n",
    "        # Identify label column\n",
    "        self.label_col = None\n",
    "        for col in self.dataset.column_names:\n",
    "            if 'label' in col.lower():\n",
    "                self.label_col = col\n",
    "                break\n",
    "        \n",
    "        print(f\"[INFO] Dataset initialized with {len(self.dataset)} samples\")\n",
    "        if self.label_col:\n",
    "            print(f\"[INFO] Using '{self.label_col}' as label column\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        \n",
    "        # Extract image/video frame\n",
    "        # This depends on your dataset structure - adapt as needed\n",
    "        if 'video' in sample:\n",
    "            # For video data, extract first frame\n",
    "            video = sample['video']\n",
    "            if isinstance(video, dict) and 'path' in video:\n",
    "                image = self._load_video_frame(video['path'])\n",
    "            else:\n",
    "                # Handle other video formats\n",
    "                image = self._extract_frame(video)\n",
    "        elif 'image' in sample:\n",
    "            image = sample['image']\n",
    "            if not isinstance(image, Image.Image):\n",
    "                image = Image.fromarray(image)\n",
    "        else:\n",
    "            # Fallback: try to find any image-like data\n",
    "            for key, value in sample.items():\n",
    "                if isinstance(value, (Image.Image, np.ndarray)):\n",
    "                    image = value if isinstance(value, Image.Image) else Image.fromarray(value)\n",
    "                    break\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Get label\n",
    "        if self.label_col:\n",
    "            label = sample[self.label_col]\n",
    "            # Convert string labels to integers if needed\n",
    "            if isinstance(label, str):\n",
    "                label = 1 if label.lower() in ['fake', 'deepfake', '1'] else 0\n",
    "        else:\n",
    "            label = 0  # Default if no label found\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def _load_video_frame(self, video_path):\n",
    "        \"\"\"Load first frame from video file\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        \n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            return Image.fromarray(frame)\n",
    "        else:\n",
    "            # Return black image if failed\n",
    "            return Image.new('RGB', (224, 224))\n",
    "    \n",
    "    def _extract_frame(self, video_data):\n",
    "        \"\"\"Extract frame from video data\"\"\"\n",
    "        # Implement based on your video data format\n",
    "        # This is a placeholder\n",
    "        return Image.new('RGB', (224, 224))\n",
    "\n",
    "# Define data transforms\n",
    "print(\"\\n[INFO] Defining data transformations...\")\n",
    "\n",
    "# Training transforms (with augmentation)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Validation/Test transforms (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"✓ Transforms defined\")\n",
    "print(\"\\nTraining augmentations:\")\n",
    "print(\"  - Resize to 224x224\")\n",
    "print(\"  - Random horizontal flip\")\n",
    "print(\"  - Random rotation (±10°)\")\n",
    "print(\"  - Color jitter\")\n",
    "print(\"  - Normalization (ImageNet stats)\")\n",
    "\n",
    "# Create datasets\n",
    "print(\"\\n[INFO] Creating datasets...\")\n",
    "\n",
    "# Start with smaller sample for testing\n",
    "MAX_SAMPLES_TEST = 100  # Set to None for full dataset\n",
    "\n",
    "try:\n",
    "    # Create full dataset\n",
    "    full_dataset = DeepfakeDataset(\n",
    "        train_data, \n",
    "        transform=train_transform,\n",
    "        max_samples=MAX_SAMPLES_TEST\n",
    "    )\n",
    "    \n",
    "    # Split into train/val/test (70/15/15)\n",
    "    train_size = int(0.7 * len(full_dataset))\n",
    "    val_size = int(0.15 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size - val_size\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        full_dataset, \n",
    "        [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(SEED)\n",
    "    )\n",
    "    \n",
    "    # Apply validation transform to val and test sets\n",
    "    val_dataset.dataset.transform = val_transform\n",
    "    test_dataset.dataset.transform = val_transform\n",
    "    \n",
    "    print(f\"\\n✓ Datasets created successfully!\")\n",
    "    print(f\"  - Training samples: {len(train_dataset)}\")\n",
    "    print(f\"  - Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"  - Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Error creating datasets: {e}\")\n",
    "    print(\"\\nNote: Adjust the dataset loading based on actual data structure\")\n",
    "\n",
    "# Create data loaders\n",
    "print(\"\\n[INFO] Creating data loaders...\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"✓ Data loaders created\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Training batches: {len(train_loader)}\")\n",
    "print(f\"  - Validation batches: {len(val_loader)}\")\n",
    "print(f\"  - Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Visualize sample batch\n",
    "print(\"\\n[INFO] Visualizing sample batch...\")\n",
    "\n",
    "def show_batch(dataloader, n_images=8):\n",
    "    \"\"\"Display a batch of images with labels\"\"\"\n",
    "    batch = next(iter(dataloader))\n",
    "    images, labels = batch\n",
    "    \n",
    "    # Denormalize images\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    images = images * std + mean\n",
    "    images = torch.clamp(images, 0, 1)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(min(n_images, len(images))):\n",
    "        img = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "        axes[i].imshow(img)\n",
    "        label_text = \"FAKE\" if labels[i].item() == 1 else \"REAL\"\n",
    "        color = 'red' if labels[i].item() == 1 else 'green'\n",
    "        axes[i].set_title(f'Label: {label_text}', fontsize=12, \n",
    "                         fontweight='bold', color=color)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sample_batch.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "try:\n",
    "    show_batch(train_loader)\n",
    "    print(\"✓ Sample batch visualization saved\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not visualize batch: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: MODELING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: MODELING - BUILDING NEURAL NETWORKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Model 1: Simple CNN (Baseline)\n",
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"Simple CNN baseline model\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # Conv Block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Conv Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Conv Block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Conv Block 4\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256 * 14 * 14, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Model 2: ResNet18 (Transfer Learning)\n",
    "def create_resnet18(num_classes=2, pretrained=True):\n",
    "    \"\"\"Create ResNet18 model with custom classifier\"\"\"\n",
    "    model = models.resnet18(pretrained=pretrained)\n",
    "    \n",
    "    # Freeze early layers (optional)\n",
    "    # for param in model.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    \n",
    "    # Replace final layer\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(num_features, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Model 3: EfficientNet-B0 (State-of-the-art)\n",
    "def create_efficientnet(num_classes=2, pretrained=True):\n",
    "    \"\"\"Create EfficientNet-B0 model\"\"\"\n",
    "    model = timm.create_model('efficientnet_b0', pretrained=pretrained)\n",
    "    \n",
    "    # Replace classifier\n",
    "    num_features = model.classifier.in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(num_features, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                num_epochs=10, device='cuda', model_name='model'):\n",
    "    \"\"\"\n",
    "    Train a model and track metrics\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "        history: Training history dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n[INFO] Training {model_name}...\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Epochs: {num_epochs}\")\n",
    "    print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': [],\n",
    "        'epoch_times': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_wts = model.state_dict()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 40)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc='Training')\n",
    "        for inputs, labels in train_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100 * train_correct / train_total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        epoch_train_loss = train_loss / train_total\n",
    "        epoch_train_acc = train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc='Validation')\n",
    "            for inputs, labels in val_pbar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "                \n",
    "                val_pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'acc': f'{100 * val_correct / val_total:.2f}%'\n",
    "                })\n",
    "        \n",
    "        epoch_val_loss = val_loss / val_total\n",
    "        epoch_val_acc = val_correct / val_total\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['train_acc'].append(epoch_train_acc)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['val_acc'].append(epoch_val_acc)\n",
    "        history['epoch_times'].append(epoch_time)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f'\\nEpoch Summary:')\n",
    "        print(f'  Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc*100:.2f}%')\n",
    "        print(f'  Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc*100:.2f}%')\n",
    "        print(f'  Time: {epoch_time:.2f}s')\n",
    "        \n",
    "        # Save best model\n",
    "        if epoch_val_acc > best_val_acc:\n",
    "            best_val_acc = epoch_val_acc\n",
    "            best_model_wts = model.state_dict()\n",
    "            print(f'  ✓ New best model! (Val Acc: {best_val_acc*100:.2f}%)')\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    print(f'\\n{\"=\"*80}')\n",
    "    print(f'Training completed!')\n",
    "    print(f'Best validation accuracy: {best_val_acc*100:.2f}%')\n",
    "    print(f'{\"=\"*80}\\n')\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, device='cuda', model_name='model'):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set and return detailed metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n[INFO] Evaluating {model_name} on test set...\")\n",
    "    \n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc='Testing'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # ROC AUC\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(all_labels, all_probs[:, 1])\n",
    "    except:\n",
    "        roc_auc = 0.0\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TEST RESULTS - {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nOverall Metrics:\")\n",
    "    print(f\"  Accuracy:  {accuracy*100:.2f}%\")\n",
    "    print(f\"  Precision: {precision*100:.2f}%\")\n",
    "    print(f\"  Recall:    {recall*100:.2f}%\")\n",
    "    print(f\"  F1-Score:  {f1*100:.2f}%\")\n",
    "    print(f\"  ROC-AUC:   {roc_auc:.4f}\")\n",
    "    \n",
    "    print(f\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, \n",
    "                                target_names=['REAL', 'FAKE']))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['REAL', 'FAKE'],\n",
    "                yticklabels=['REAL', 'FAKE'])\n",
    "    plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'confusion_matrix_{model_name}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    if roc_auc > 0:\n",
    "        fpr, tpr, _ = roc_curve(all_labels, all_probs[:, 1])\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, linewidth=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
    "        plt.xlabel('False Positive Rate', fontsize=12)\n",
    "        plt.ylabel('True Positive Rate', fontsize=12)\n",
    "        plt.title(f'ROC Curve - {model_name}', fontsize=14, fontweight='bold')\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'roc_curve_{model_name}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Visualization function for training history\n",
    "def plot_training_history(history, model_name='model'):\n",
    "    \"\"\"Plot training and validation metrics\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(epochs, history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "    axes[0].plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "    axes[0].set_title(f'Training and Validation Loss - {model_name}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[1].plot(epochs, [acc*100 for acc in history['train_acc']], \n",
    "                'b-', label='Training Accuracy', linewidth=2)\n",
    "    axes[1].plot(epochs, [acc*100 for acc in history['val_acc']], \n",
    "                'r-', label='Validation Accuracy', linewidth=2)\n",
    "    axes[1].set_title(f'Training and Validation Accuracy - {model_name}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'training_history_{model_name}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING MULTIPLE MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Dictionary to store all results\n",
    "all_results = {}\n",
    "\n",
    "# Hyperparameters\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Model 1: Simple CNN\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 1: SIMPLE CNN (BASELINE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    simple_cnn = SimpleCNN(num_classes=2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(simple_cnn.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    simple_cnn, history_simple = train_model(\n",
    "        simple_cnn, train_loader, val_loader, \n",
    "        criterion, optimizer, \n",
    "        num_epochs=NUM_EPOCHS, \n",
    "        device=device,\n",
    "        model_name='SimpleCNN'\n",
    "    )\n",
    "    \n",
    "    plot_training_history(history_simple, 'SimpleCNN')\n",
    "    metrics_simple = evaluate_model(simple_cnn, test_loader, device, 'SimpleCNN')\n",
    "    all_results['SimpleCNN'] = metrics_simple\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error training SimpleCNN: {e}\")\n",
    "\n",
    "# Model 2: ResNet18\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 2: RESNET18 (TRANSFER LEARNING)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    resnet18 = create_resnet18(num_classes=2, pretrained=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(resnet18.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    resnet18, history_resnet = train_model(\n",
    "        resnet18, train_loader, val_loader,\n",
    "        criterion, optimizer,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        device=device,\n",
    "        model_name='ResNet18'\n",
    "    )\n",
    "    \n",
    "    plot_training_history(history_resnet, 'ResNet18')\n",
    "    metrics_resnet = evaluate_model(resnet18, test_loader, device, 'ResNet18')\n",
    "    all_results['ResNet18'] = metrics_resnet\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error training ResNet18: {e}\")\n",
    "\n",
    "# Model 3: EfficientNet-B0\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 3: EFFICIENTNET-B0 (STATE-OF-THE-ART)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    efficientnet = create_efficientnet(num_classes=2, pretrained=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(efficientnet.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    efficientnet, history_eff = train_model(\n",
    "        efficientnet, train_loader, val_loader,\n",
    "        criterion, optimizer,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        device=device,\n",
    "        model_name='EfficientNet-B0'\n",
    "    )\n",
    "    \n",
    "    plot_training_history(history_eff, 'EfficientNet-B0')\n",
    "    metrics_eff = evaluate_model(efficientnet, test_loader, device, 'EfficientNet-B0')\n",
    "    all_results['EfficientNet-B0'] = metrics_eff\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error training EfficientNet: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: MODEL COMPARISON & SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: MODEL COMPARISON & SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    model_name: {\n",
    "        'Accuracy (%)': metrics['accuracy'] * 100,\n",
    "        'Precision (%)': metrics['precision'] * 100,\n",
    "        'Recall (%)': metrics['recall'] * 100,\n",
    "        'F1-Score (%)': metrics['f1_score'] * 100,\n",
    "        'ROC-AUC': metrics['roc_auc']\n",
    "    }\n",
    "    for model_name, metrics in all_results.items()\n",
    "}).T\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"MODEL COMPARISON TABLE\")\n",
    "print(\"-\"*80)\n",
    "print(comparison_df.to_string())\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics_to_plot = ['Accuracy (%)', 'Precision (%)', 'Recall (%)', 'F1-Score (%)']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    comparison_df[metric].plot(kind='bar', ax=ax, color=colors)\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel(metric, fontsize=12)\n",
    "    ax.set_xlabel('Model', fontsize=12)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.2f', padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Select best model\n",
    "best_model_name = comparison_df['F1-Score (%)'].idxmax()\n",
    "best_f1_score = comparison_df.loc[best_model_name, 'F1-Score (%)']\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BEST MODEL SELECTED: {best_model_name}\")\n",
    "print(f\"F1-Score: {best_f1_score:.2f}%\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: HYPERPARAMETER TUNING (Optional - on best model)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n[INFO] Hyperparameter tuning can be performed on the best model\")\n",
    "print(\"Consider tuning:\")\n",
    "print(\"  - Learning rate\")\n",
    "print(\"  - Batch size\")\n",
    "print(\"  - Optimizer (Adam, SGD, AdamW)\")\n",
    "print(\"  - Number of epochs\")\n",
    "print(\"  - Dropout rates\")\n",
    "print(\"  - Data augmentation parameters\")\n",
    "print(\"\\nThis step is implemented in a separate notebook for grid/random search\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: FINAL REPORT & DOCUMENTATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROJECT SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"DEEPFAKE DETECTION PROJECT - FINAL SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nDate: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Dataset: Deep Fake Detection (DFD) from HuggingFace\")\n",
    "print(f\"Total Samples: {len(train_data)}\")\n",
    "print(f\"Train/Val/Test Split: 70/15/15\")\n",
    "print(f\"\\nModels Trained: {len(all_results)}\")\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Best F1-Score: {best_f1_score:.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"KEY FINDINGS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"\\n1. Data Quality:\")\n",
    "print(\"   - Dataset loaded successfully from HuggingFace\")\n",
    "print(\"   - Class distribution analyzed\")\n",
    "print(\"   - Data augmentation applied\")\n",
    "\n",
    "print(\"\\n2. Model Performance:\")\n",
    "print(comparison_df.to_string())\n",
    "\n",
    "print(\"\\n3. Recommendations:\")\n",
    "if best_f1_score >= 90:\n",
    "    print(\"   ✓ Model meets target performance (F1 > 90%)\")\n",
    "else:\n",
    "    print(\"   → Consider additional training epochs\")\n",
    "    print(\"   → Try ensemble methods\")\n",
    "    print(\"   → Collect more training data\")\n",
    "\n",
    "print(\"\\n4. Next Steps:\")\n",
    "print(\"   - Deploy best model\")\n",
    "print(\"   - Implement real-time video detection\")\n",
    "print(\"   - Add explainability (Grad-CAM)\")\n",
    "print(\"   - Write IEEE conference paper\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Save results\n",
    "results_file = 'model_results.txt'\n",
    "with open(results_file, 'w') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"DEEPFAKE DETECTION - MODEL RESULTS\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    f.write(comparison_df.to_string())\n",
    "    f.write(f\"\\n\\nBest Model: {best_model_name}\\n\")\n",
    "    f.write(f\"Best F1-Score: {best_f1_score:.2f}%\\n\")\n",
    "\n",
    "print(f\"\\n[INFO] Results saved to '{results_file}'\")\n",
    "\n",
    "# ============================================================================\n",
    "# END OF SCRIPT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL TASKS COMPLETED!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated Files:\")\n",
    "print(\"  - label_distribution.png\")\n",
    "print(\"  - sample_batch.png\")\n",
    "print(\"  - training_history_*.png\")\n",
    "print(\"  - confusion_matrix_*.png\")\n",
    "print(\"  - roc_curve_*.png\")\n",
    "print(\"  - model_comparison.png\")\n",
    "print(\"  - model_results.txt\")\n",
    "print(\"\\nNext: Review results and proceed with IEEE paper writing\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
