{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa5b4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./protein_env/lib/python3.12/site-packages (25.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Jupyter Notebook: Explore label features that influence Sales (regression + classification)\n",
    "# Author: Mohini\n",
    "\n",
    "# ---------- 0. Environment / Installs ----------\n",
    "# Run this cell if you need to install packages. Comment out if already installed.\n",
    "%pip install --upgrade pip\n",
    "%pip install --quiet scikit-image shap xgboost tensorflow keras opencv-python-headless matplotlib\n",
    "#%pip install --upgrade certifi\n",
    "\n",
    "# ---------- 1. Imports ----------\n",
    "import os, zipfile, math, io, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image, ImageStat\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import Pipeline\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import hog\n",
    "from skimage.filters import sobel\n",
    "from skimage import io as skio\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "\n",
    "# Fix SSL certificate issue\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image as kimage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c233ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE EXTRACTION VISUALIZATIONS FOR THESIS\n",
    "# ============================================================================\n",
    "# Comprehensive charts for understanding extracted features\n",
    "# These visualizations help interpret the feature extraction results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.colors import rgb2hex\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for publication-quality figures\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except:\n",
    "    try:\n",
    "        plt.style.use('seaborn-darkgrid')\n",
    "    except:\n",
    "        plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 9\n",
    "plt.rcParams['ytick.labelsize'] = 9\n",
    "plt.rcParams['legend.fontsize'] = 9\n",
    "\n",
    "# Ensure we have the full dataframe with features\n",
    "# This assumes features_df and full have been created in previous cells\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE EXTRACTION VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. BASIC IMAGE STATISTICS VISUALIZATION\n",
    "# ============================================================================\n",
    "print(\"\\n1. Creating Basic Image Statistics Charts...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Basic Image Statistics: RGB Channel Analysis', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 1.1 RGB Mean Values Comparison\n",
    "ax1 = axes[0, 0]\n",
    "rgb_means = full[['mean_r', 'mean_g', 'mean_b']].values\n",
    "x_pos = np.arange(len(full))\n",
    "width = 0.25\n",
    "ax1.bar(x_pos - width, full['mean_r'], width, label='Red', color='#FF6B6B', alpha=0.8)\n",
    "ax1.bar(x_pos, full['mean_g'], width, label='Green', color='#4ECDC4', alpha=0.8)\n",
    "ax1.bar(x_pos + width, full['mean_b'], width, label='Blue', color='#45B7D1', alpha=0.8)\n",
    "ax1.set_xlabel('Product Index')\n",
    "ax1.set_ylabel('Mean RGB Value (0-255)')\n",
    "ax1.set_title('RGB Mean Values Across Products')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels([f'P{i+1}' for i in range(len(full))], rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 1.2 RGB Standard Deviation Comparison\n",
    "ax2 = axes[0, 1]\n",
    "ax2.bar(x_pos - width, full['std_r'], width, label='Red Std', color='#FF6B6B', alpha=0.8)\n",
    "ax2.bar(x_pos, full['std_g'], width, label='Green Std', color='#4ECDC4', alpha=0.8)\n",
    "ax2.bar(x_pos + width, full['std_b'], width, label='Blue Std', color='#45B7D1', alpha=0.8)\n",
    "ax2.set_xlabel('Product Index')\n",
    "ax2.set_ylabel('RGB Standard Deviation')\n",
    "ax2.set_title('RGB Standard Deviation (Color Variation)')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels([f'P{i+1}' for i in range(len(full))], rotation=45, ha='right')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 1.3 RGB Mean Distribution\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(full['mean_r'], bins=15, alpha=0.6, label='Red', color='#FF6B6B', edgecolor='black')\n",
    "ax3.hist(full['mean_g'], bins=15, alpha=0.6, label='Green', color='#4ECDC4', edgecolor='black')\n",
    "ax3.hist(full['mean_b'], bins=15, alpha=0.6, label='Blue', color='#45B7D1', edgecolor='black')\n",
    "ax3.set_xlabel('Mean RGB Value')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title('Distribution of RGB Mean Values')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 1.4 RGB Std vs Mean Scatter\n",
    "ax4 = axes[1, 1]\n",
    "scatter1 = ax4.scatter(full['mean_r'], full['std_r'], c='#FF6B6B', s=100, alpha=0.6, label='Red', edgecolors='black', linewidth=0.5)\n",
    "scatter2 = ax4.scatter(full['mean_g'], full['std_g'], c='#4ECDC4', s=100, alpha=0.6, label='Green', edgecolors='black', linewidth=0.5)\n",
    "scatter3 = ax4.scatter(full['mean_b'], full['std_b'], c='#45B7D1', s=100, alpha=0.6, label='Blue', edgecolors='black', linewidth=0.5)\n",
    "ax4.set_xlabel('Mean RGB Value')\n",
    "ax4.set_ylabel('Standard Deviation')\n",
    "ax4.set_title('Mean vs Standard Deviation by Channel')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '01_basic_image_statistics.png', bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. COLOR FEATURE VISUALIZATION\n",
    "# ============================================================================\n",
    "print(\"\\n2. Creating Color Feature Charts...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Color Feature Analysis: Dominant Colors and Hue Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2.1 Dominant Colors Visualization\n",
    "ax1 = axes[0, 0]\n",
    "# Extract dominant colors (first 3 colors, each with RGB values)\n",
    "n_products = len(full)\n",
    "colors_per_product = 3\n",
    "color_swatches = []\n",
    "\n",
    "for idx in range(min(10, n_products)):  # Show first 10 products\n",
    "    # Get RGB values for 3 dominant colors\n",
    "    r1, g1, b1 = full.iloc[idx]['color_feat_0'], full.iloc[idx]['color_feat_1'], full.iloc[idx]['color_feat_2']\n",
    "    r2, g2, b2 = full.iloc[idx]['color_feat_3'], full.iloc[idx]['color_feat_4'], full.iloc[idx]['color_feat_5']\n",
    "    r3, g3, b3 = full.iloc[idx]['color_feat_6'], full.iloc[idx]['color_feat_7'], full.iloc[idx]['color_feat_8']\n",
    "    \n",
    "    # Get coverage percentages\n",
    "    pct1 = full.iloc[idx]['color_feat_9']\n",
    "    pct2 = full.iloc[idx]['color_feat_10']\n",
    "    pct3 = full.iloc[idx]['color_feat_11']\n",
    "    \n",
    "    # Normalize RGB to 0-1\n",
    "    colors = [\n",
    "        (r1/255, g1/255, b1/255),\n",
    "        (r2/255, g2/255, b2/255),\n",
    "        (r3/255, g3/255, b3/255)\n",
    "    ]\n",
    "    \n",
    "    y_pos = idx * 0.8\n",
    "    x_start = 0\n",
    "    for i, (color, pct) in enumerate(zip(colors, [pct1, pct2, pct3])):\n",
    "        width = pct * 0.8\n",
    "        rect = Rectangle((x_start, y_pos), width, 0.6, facecolor=color, edgecolor='black', linewidth=0.5)\n",
    "        ax1.add_patch(rect)\n",
    "        x_start += width\n",
    "    \n",
    "    ax1.text(-0.05, y_pos + 0.3, f'P{idx+1}', ha='right', va='center', fontsize=8)\n",
    "\n",
    "ax1.set_xlim(-0.2, 0.85)\n",
    "ax1.set_ylim(-0.2, min(10, n_products) * 0.8)\n",
    "ax1.set_xlabel('Color Coverage (Proportional)')\n",
    "ax1.set_ylabel('Product Index')\n",
    "ax1.set_title('Dominant Color Palettes (Top 3 Colors per Product)')\n",
    "ax1.set_yticks([i*0.8 + 0.3 for i in range(min(10, n_products))])\n",
    "ax1.set_yticklabels([f'P{i+1}' for i in range(min(10, n_products))])\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# 2.2 Hue Histogram Comparison\n",
    "ax2 = axes[0, 1]\n",
    "hue_bins = 12\n",
    "hue_data = np.array([full[f'color_feat_{i+12}'].values for i in range(hue_bins)])\n",
    "x_pos_hue = np.arange(hue_bins)\n",
    "width_hue = 0.8 / n_products\n",
    "\n",
    "for idx in range(n_products):\n",
    "    offset = (idx - n_products/2) * width_hue\n",
    "    ax2.bar(x_pos_hue + offset, hue_data[:, idx], width_hue, alpha=0.7, \n",
    "            label=f'P{idx+1}' if idx < 5 else '')\n",
    "\n",
    "ax2.set_xlabel('Hue Bin (0-360°)')\n",
    "ax2.set_ylabel('Normalized Frequency')\n",
    "ax2.set_title('Hue Distribution Across Products')\n",
    "ax2.set_xticks(x_pos_hue)\n",
    "ax2.set_xticklabels([f'{i*30}°' for i in range(hue_bins)], rotation=45)\n",
    "ax2.legend(ncol=2, fontsize=7)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2.3 Average Dominant Color Coverage\n",
    "ax3 = axes[1, 0]\n",
    "coverage_data = [full['color_feat_9'].mean(), full['color_feat_10'].mean(), full['color_feat_11'].mean()]\n",
    "colors_avg = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "bars = ax3.bar(['Color 1', 'Color 2', 'Color 3'], coverage_data, color=colors_avg, alpha=0.8, edgecolor='black')\n",
    "ax3.set_ylabel('Average Coverage Percentage')\n",
    "ax3.set_title('Average Coverage of Dominant Colors')\n",
    "ax3.set_ylim(0, max(coverage_data) * 1.2)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, coverage_data):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2.4 Color Diversity (Hue Histogram Entropy)\n",
    "ax4 = axes[1, 1]\n",
    "from scipy.stats import entropy\n",
    "\n",
    "hue_entropies = []\n",
    "for idx in range(n_products):\n",
    "    hue_hist = [full.iloc[idx][f'color_feat_{i+12}'] for i in range(hue_bins)]\n",
    "    # Normalize to probability distribution\n",
    "    hue_hist = np.array(hue_hist)\n",
    "    hue_hist = hue_hist / hue_hist.sum() if hue_hist.sum() > 0 else hue_hist\n",
    "    ent = entropy(hue_hist + 1e-10)  # Add small value to avoid log(0)\n",
    "    hue_entropies.append(ent)\n",
    "\n",
    "ax4.bar(range(n_products), hue_entropies, color='#95E1D3', alpha=0.8, edgecolor='black')\n",
    "ax4.set_xlabel('Product Index')\n",
    "ax4.set_ylabel('Hue Entropy (Color Diversity)')\n",
    "ax4.set_title('Color Diversity Score (Higher = More Diverse)')\n",
    "ax4.set_xticks(range(n_products))\n",
    "ax4.set_xticklabels([f'P{i+1}' for i in range(n_products)], rotation=45, ha='right')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '02_color_features.png', bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. TEXTURE & GRAPHICS FEATURE VISUALIZATION\n",
    "# ============================================================================\n",
    "print(\"\\n3. Creating Texture & Graphics Feature Charts...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Texture & Graphics Feature Analysis', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 3.1 Edge Density Distribution\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(full['edge_density'], bins=15, color='#FF6B6B', alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(full['edge_density'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {full[\"edge_density\"].mean():.3f}')\n",
    "ax1.set_xlabel('Edge Density (0-1)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Edge Density Distribution\\n(Higher = More Complex/Busy Design)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 3.2 HOG Mean vs Edge Density\n",
    "ax2 = axes[0, 1]\n",
    "scatter = ax2.scatter(full['hog_mean'], full['edge_density'], \n",
    "                     c=full['Sale'] if 'Sale' in full.columns else range(len(full)),\n",
    "                     s=150, alpha=0.7, cmap='viridis', edgecolors='black', linewidth=0.5)\n",
    "ax2.set_xlabel('HOG Mean (Shape/Edge Patterns)')\n",
    "ax2.set_ylabel('Edge Density')\n",
    "ax2.set_title('HOG vs Edge Density Relationship')\n",
    "if 'Sale' in full.columns:\n",
    "    cbar = plt.colorbar(scatter, ax=ax2)\n",
    "    cbar.set_label('Sales', rotation=270, labelpad=15)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3.3 LBP Histogram (Average across all products)\n",
    "ax3 = axes[1, 0]\n",
    "lbp_cols = [f'lbp_{i}' for i in range(16)]\n",
    "lbp_avg = [full[col].mean() for col in lbp_cols]\n",
    "bars = ax3.bar(range(16), lbp_avg, color='#4ECDC4', alpha=0.8, edgecolor='black')\n",
    "ax3.set_xlabel('LBP Pattern Bin')\n",
    "ax3.set_ylabel('Average Normalized Frequency')\n",
    "ax3.set_title('Average Local Binary Pattern (LBP) Histogram\\n(Texture Pattern Distribution)')\n",
    "ax3.set_xticks(range(16))\n",
    "ax3.set_xticklabels([f'B{i}' for i in range(16)], rotation=45, ha='right')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3.4 Texture Complexity Score (Combined)\n",
    "ax4 = axes[1, 1]\n",
    "# Create a combined texture complexity score\n",
    "texture_complexity = (full['edge_density'] * 0.4 + \n",
    "                     (full['hog_mean'] / full['hog_mean'].max()) * 0.3 +\n",
    "                     (full[[f'lbp_{i}' for i in range(16)]].sum(axis=1) / \n",
    "                      full[[f'lbp_{i}' for i in range(16)]].sum(axis=1).max()) * 0.3)\n",
    "\n",
    "ax4.bar(range(n_products), texture_complexity, color='#95E1D3', alpha=0.8, edgecolor='black')\n",
    "ax4.set_xlabel('Product Index')\n",
    "ax4.set_ylabel('Texture Complexity Score')\n",
    "ax4.set_title('Combined Texture Complexity\\n(Edge Density + HOG + LBP)')\n",
    "ax4.set_xticks(range(n_products))\n",
    "ax4.set_xticklabels([f'P{i+1}' for i in range(n_products)], rotation=45, ha='right')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '03_texture_features.png', bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 4. LAYOUT & LOGO FEATURE VISUALIZATION\n",
    "# ============================================================================\n",
    "print(\"\\n4. Creating Layout & Logo Feature Charts...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Layout & Logo Feature Analysis', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 4.1 Aspect Ratio Distribution\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(full['aspect_ratio'], bins=15, color='#FF6B6B', alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(1.0, color='red', linestyle='--', linewidth=2, label='Square (1.0)')\n",
    "ax1.axvline(full['aspect_ratio'].mean(), color='blue', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: {full[\"aspect_ratio\"].mean():.3f}')\n",
    "ax1.set_xlabel('Aspect Ratio (Width/Height)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Aspect Ratio Distribution')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 4.2 White Space Percentage\n",
    "ax2 = axes[0, 1]\n",
    "ax2.bar(range(n_products), full['white_pct'], color='#4ECDC4', alpha=0.8, edgecolor='black')\n",
    "ax2.set_xlabel('Product Index')\n",
    "ax2.set_ylabel('White Space Percentage')\n",
    "ax2.set_title('White Space Usage\\n(Higher = More Minimalist Design)')\n",
    "ax2.set_xticks(range(n_products))\n",
    "ax2.set_xticklabels([f'P{i+1}' for i in range(n_products)], rotation=45, ha='right')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4.3 Logo Score Distribution\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(full['logo_score'], bins=15, color='#45B7D1', alpha=0.7, edgecolor='black')\n",
    "ax3.axvline(full['logo_score'].mean(), color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: {full[\"logo_score\"].mean():.4f}')\n",
    "ax3.set_xlabel('Logo Score')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title('Logo Prominence Score Distribution\\n(Higher = More Prominent Branding)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4.4 Layout Design Space (White Space vs Logo Score)\n",
    "ax4 = axes[1, 1]\n",
    "scatter = ax4.scatter(full['white_pct'], full['logo_score'],\n",
    "                     c=full['Sale'] if 'Sale' in full.columns else range(len(full)),\n",
    "                     s=150, alpha=0.7, cmap='plasma', edgecolors='black', linewidth=0.5)\n",
    "ax4.set_xlabel('White Space Percentage')\n",
    "ax4.set_ylabel('Logo Score')\n",
    "ax4.set_title('Design Space: White Space vs Logo Prominence')\n",
    "if 'Sale' in full.columns:\n",
    "    cbar = plt.colorbar(scatter, ax=ax4)\n",
    "    cbar.set_label('Sales', rotation=270, labelpad=15)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '04_layout_logo_features.png', bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 5. TYPOGRAPHY FEATURE VISUALIZATION\n",
    "# ============================================================================\n",
    "print(\"\\n5. Creating Typography Feature Charts...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Typography Feature Analysis', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 5.1 Text Percentage Distribution\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(full['text_pct'], bins=15, color='#FF6B6B', alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(full['text_pct'].mean(), color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: {full[\"text_pct\"].mean():.3f}')\n",
    "ax1.set_xlabel('Text Percentage (0-1)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Text Coverage Distribution\\n(Higher = More Information-Dense)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 5.2 Text Count Distribution\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(full['text_cnts'], bins=15, color='#4ECDC4', alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(full['text_cnts'].mean(), color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: {full[\"text_cnts\"].mean():.1f}')\n",
    "ax2.set_xlabel('Number of Text Regions')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Text Region Count Distribution\\n(Higher = More Text Blocks)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 5.3 Text Percentage vs Text Count\n",
    "ax3 = axes[1, 0]\n",
    "scatter = ax3.scatter(full['text_pct'], full['text_cnts'],\n",
    "                     c=full['Sale'] if 'Sale' in full.columns else range(len(full)),\n",
    "                     s=150, alpha=0.7, cmap='coolwarm', edgecolors='black', linewidth=0.5)\n",
    "ax3.set_xlabel('Text Percentage')\n",
    "ax3.set_ylabel('Text Region Count')\n",
    "ax3.set_title('Typography Relationship: Coverage vs Complexity')\n",
    "if 'Sale' in full.columns:\n",
    "    cbar = plt.colorbar(scatter, ax=ax3)\n",
    "    cbar.set_label('Sales', rotation=270, labelpad=15)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 5.4 Typography Complexity Score (Combined)\n",
    "ax4 = axes[1, 1]\n",
    "# Normalize and combine text features\n",
    "text_pct_norm = (full['text_pct'] - full['text_pct'].min()) / (full['text_pct'].max() - full['text_pct'].min() + 1e-10)\n",
    "text_cnts_norm = (full['text_cnts'] - full['text_cnts'].min()) / (full['text_cnts'].max() - full['text_cnts'].min() + 1e-10)\n",
    "typography_complexity = (text_pct_norm * 0.6 + text_cnts_norm * 0.4)\n",
    "\n",
    "ax4.bar(range(n_products), typography_complexity, color='#95E1D3', alpha=0.8, edgecolor='black')\n",
    "ax4.set_xlabel('Product Index')\n",
    "ax4.set_ylabel('Typography Complexity Score')\n",
    "ax4.set_title('Combined Typography Complexity\\n(Text Coverage + Text Blocks)')\n",
    "ax4.set_xticks(range(n_products))\n",
    "ax4.set_xticklabels([f'P{i+1}' for i in range(n_products)], rotation=45, ha='right')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '05_typography_features.png', bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 6. COMPREHENSIVE FEATURE SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n6. Creating Comprehensive Feature Summary...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Comprehensive Feature Extraction Summary', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 6.1 Feature Categories Overview\n",
    "ax1 = axes[0, 0]\n",
    "feature_categories = {\n",
    "    'Basic Stats': 6,  # mean_r, mean_g, mean_b, std_r, std_g, std_b\n",
    "    'Color': 24,       # color_feat_0 to color_feat_23\n",
    "    'Texture': 18,     # hog_mean, edge_density, lbp_0 to lbp_15\n",
    "    'Layout/Logo': 3,  # aspect_ratio, white_pct, logo_score\n",
    "    'Typography': 2,  # text_pct, text_cnts\n",
    "    'Embeddings': 64   # emb_0 to emb_63\n",
    "}\n",
    "colors_cat = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#95E1D3', '#F38181', '#AA96DA']\n",
    "bars = ax1.bar(feature_categories.keys(), feature_categories.values(), \n",
    "               color=colors_cat[:len(feature_categories)], alpha=0.8, edgecolor='black')\n",
    "ax1.set_ylabel('Number of Features')\n",
    "ax1.set_title('Feature Count by Category')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "for bar, val in zip(bars, feature_categories.values()):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{val}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 6.2 Feature Value Ranges (Normalized)\n",
    "ax2 = axes[0, 1]\n",
    "# Sample key features to show ranges\n",
    "key_features = {\n",
    "    'mean_r': full['mean_r'].mean(),\n",
    "    'edge_density': full['edge_density'].mean() * 255,  # Scale for comparison\n",
    "    'white_pct': full['white_pct'].mean() * 255,\n",
    "    'text_pct': full['text_pct'].mean() * 255,\n",
    "    'logo_score': full['logo_score'].mean() * 10000  # Scale for visibility\n",
    "}\n",
    "bars = ax2.bar(key_features.keys(), key_features.values(), \n",
    "               color='#95E1D3', alpha=0.8, edgecolor='black')\n",
    "ax2.set_ylabel('Average Value (Normalized)')\n",
    "ax2.set_title('Key Feature Average Values')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 6.3 Feature Correlation Heatmap (Sample)\n",
    "ax3 = axes[0, 2]\n",
    "sample_features = ['mean_r', 'mean_g', 'mean_b', 'edge_density', 'white_pct', \n",
    "                   'logo_score', 'text_pct', 'text_cnts', 'hog_mean']\n",
    "corr_data = full[sample_features].corr()\n",
    "im = ax3.imshow(corr_data, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "ax3.set_xticks(range(len(sample_features)))\n",
    "ax3.set_yticks(range(len(sample_features)))\n",
    "ax3.set_xticklabels(sample_features, rotation=45, ha='right')\n",
    "ax3.set_yticklabels(sample_features)\n",
    "ax3.set_title('Feature Correlation Matrix')\n",
    "plt.colorbar(im, ax=ax3, label='Correlation')\n",
    "\n",
    "# 6.4 Feature Distribution Comparison (Box Plot)\n",
    "ax4 = axes[1, 0]\n",
    "sample_data = [full['mean_r'], full['edge_density']*255, full['white_pct']*255, \n",
    "               full['text_pct']*255]\n",
    "bp = ax4.boxplot(sample_data, labels=['RGB Mean', 'Edge Density', 'White %', 'Text %'],\n",
    "                 patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], ['#FF6B6B', '#4ECDC4', '#45B7D1', '#95E1D3']):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "ax4.set_ylabel('Normalized Value')\n",
    "ax4.set_title('Feature Distribution Comparison')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 6.5 Feature Importance by Category (if Sale column exists)\n",
    "ax5 = axes[1, 1]\n",
    "if 'Sale' in full.columns:\n",
    "    # Calculate correlation with sales for each category\n",
    "    category_corrs = {\n",
    "        'Basic Stats': abs(full[['mean_r', 'mean_g', 'mean_b']].corrwith(full['Sale'])).mean(),\n",
    "        'Color': abs(full[[f'color_feat_{i}' for i in range(24)]].corrwith(full['Sale'])).mean(),\n",
    "        'Texture': abs(full[['edge_density', 'hog_mean'] + [f'lbp_{i}' for i in range(16)]].corrwith(full['Sale'])).mean(),\n",
    "        'Layout': abs(full[['aspect_ratio', 'white_pct', 'logo_score']].corrwith(full['Sale'])).mean(),\n",
    "        'Typography': abs(full[['text_pct', 'text_cnts']].corrwith(full['Sale'])).mean()\n",
    "    }\n",
    "    bars = ax5.bar(category_corrs.keys(), category_corrs.values(), \n",
    "                   color=colors_cat[:len(category_corrs)], alpha=0.8, edgecolor='black')\n",
    "    ax5.set_ylabel('Average |Correlation| with Sales')\n",
    "    ax5.set_title('Feature Category Importance (Sales Correlation)')\n",
    "    ax5.tick_params(axis='x', rotation=45)\n",
    "    ax5.grid(True, alpha=0.3, axis='y')\n",
    "    for bar, val in zip(bars, category_corrs.values()):\n",
    "        height = bar.get_height()\n",
    "        ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                 f'{val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "else:\n",
    "    ax5.text(0.5, 0.5, 'Sales data not available\\nfor correlation analysis', \n",
    "            ha='center', va='center', transform=ax5.transAxes, fontsize=12)\n",
    "    ax5.set_title('Feature Category Importance')\n",
    "\n",
    "# 6.6 Feature Extraction Pipeline Summary\n",
    "ax6 = axes[1, 2]\n",
    "ax6.axis('off')\n",
    "pipeline_text = \"\"\"\n",
    "FEATURE EXTRACTION PIPELINE\n",
    "\n",
    "1. Basic Image Statistics\n",
    "   → RGB Mean & Std (6 features)\n",
    "\n",
    "2. Color Features\n",
    "   → Dominant Colors (K-Means)\n",
    "   → Color Coverage\n",
    "   → Hue Histogram (24 features)\n",
    "\n",
    "3. Texture & Graphics\n",
    "   → HOG (Histogram of Oriented Gradients)\n",
    "   → Edge Density (Sobel)\n",
    "   → LBP (Local Binary Pattern) (18 features)\n",
    "\n",
    "4. Layout & Logo\n",
    "   → Aspect Ratio\n",
    "   → White Space %\n",
    "   → Logo Score (3 features)\n",
    "\n",
    "5. Typography\n",
    "   → Text Coverage %\n",
    "   → Text Region Count (2 features)\n",
    "\n",
    "6. Deep Learning\n",
    "   → ResNet50 Embeddings (64 features)\n",
    "\n",
    "TOTAL: ~117 Features per Image\n",
    "\"\"\"\n",
    "ax6.text(0.1, 0.5, pipeline_text, transform=ax6.transAxes,\n",
    "         fontsize=10, verticalalignment='center', family='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '06_comprehensive_summary.png', bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All visualizations saved to:\", OUTPUT_DIR)\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated Charts:\")\n",
    "print(\"  1. 01_basic_image_statistics.png - RGB channel analysis\")\n",
    "print(\"  2. 02_color_features.png - Color palette and hue distribution\")\n",
    "print(\"  3. 03_texture_features.png - Texture and graphics analysis\")\n",
    "print(\"  4. 04_layout_logo_features.png - Layout and logo analysis\")\n",
    "print(\"  5. 05_typography_features.png - Typography analysis\")\n",
    "print(\"  6. 06_comprehensive_summary.png - Overall feature summary\")\n",
    "print(\"=\"*80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138cd0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE EXTRACTION VISUALIZATIONS FOR THESIS\n",
    "# ============================================================================\n",
    "# Comprehensive charts for understanding extracted features\n",
    "# These visualizations help interpret the feature extraction results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.colors import rgb2hex\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for publication-quality figures\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except:\n",
    "    try:\n",
    "        plt.style.use('seaborn-darkgrid')\n",
    "    except:\n",
    "        plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 9\n",
    "plt.rcParams['ytick.labelsize'] = 9\n",
    "plt.rcParams['legend.fontsize'] = 9\n",
    "\n",
    "# Ensure we have the full dataframe with features\n",
    "# This assumes features_df and full have been created in previous cells\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE EXTRACTION VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. BASIC IMAGE STATISTICS VISUALIZATION\n",
    "# ============================================================================\n",
    "print(\"\\n1. Creating Basic Image Statistics Charts...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Basic Image Statistics: RGB Channel Analysis', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 1.1 RGB Mean Values Comparison\n",
    "ax1 = axes[0, 0]\n",
    "rgb_means = full[['mean_r', 'mean_g', 'mean_b']].values\n",
    "x_pos = np.arange(len(full))\n",
    "width = 0.25\n",
    "ax1.bar(x_pos - width, full['mean_r'], width, label='Red', color='#FF6B6B', alpha=0.8)\n",
    "ax1.bar(x_pos, full['mean_g'], width, label='Green', color='#4ECDC4', alpha=0.8)\n",
    "ax1.bar(x_pos + width, full['mean_b'], width, label='Blue', color='#45B7D1', alpha=0.8)\n",
    "ax1.set_xlabel('Product Index')\n",
    "ax1.set_ylabel('Mean RGB Value (0-255)')\n",
    "ax1.set_title('RGB Mean Values Across Products')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels([f'P{i+1}' for i in range(len(full))], rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 1.2 RGB Standard Deviation Comparison\n",
    "ax2 = axes[0, 1]\n",
    "ax2.bar(x_pos - width, full['std_r'], width, label='Red Std', color='#FF6B6B', alpha=0.8)\n",
    "ax2.bar(x_pos, full['std_g'], width, label='Green Std', color='#4ECDC4', alpha=0.8)\n",
    "ax2.bar(x_pos + width, full['std_b'], width, label='Blue Std', color='#45B7D1', alpha=0.8)\n",
    "ax2.set_xlabel('Product Index')\n",
    "ax2.set_ylabel('RGB Standard Deviation')\n",
    "ax2.set_title('RGB Standard Deviation (Color Variation)')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels([f'P{i+1}' for i in range(len(full))], rotation=45, ha='right')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 1.3 RGB Mean Distribution\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(full['mean_r'], bins=15, alpha=0.6, label='Red', color='#FF6B6B', edgecolor='black')\n",
    "ax3.hist(full['mean_g'], bins=15, alpha=0.6, label='Green', color='#4ECDC4', edgecolor='black')\n",
    "ax3.hist(full['mean_b'], bins=15, alpha=0.6, label='Blue', color='#45B7D1', edgecolor='black')\n",
    "ax3.set_xlabel('Mean RGB Value')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title('Distribution of RGB Mean Values')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 1.4 RGB Std vs Mean Scatter\n",
    "ax4 = axes[1, 1]\n",
    "scatter1 = ax4.scatter(full['mean_r'], full['std_r'], c='#FF6B6B', s=100, alpha=0.6, label='Red', edgecolors='black', linewidth=0.5)\n",
    "scatter2 = ax4.scatter(full['mean_g'], full['std_g'], c='#4ECDC4', s=100, alpha=0.6, label='Green', edgecolors='black', linewidth=0.5)\n",
    "scatter3 = ax4.scatter(full['mean_b'], full['std_b'], c='#45B7D1', s=100, alpha=0.6, label='Blue', edgecolors='black', linewidth=0.5)\n",
    "ax4.set_xlabel('Mean RGB Value')\n",
    "ax4.set_ylabel('Standard Deviation')\n",
    "ax4.set_title('Mean vs Standard Deviation by Channel')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '01_basic_image_statistics.png', bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. COLOR FEATURE VISUALIZATION\n",
    "# ============================================================================\n",
    "print(\"\\n2. Creating Color Feature Charts...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Color Feature Analysis: Dominant Colors and Hue Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2.1 Dominant Colors Visualization\n",
    "ax1 = axes[0, 0]\n",
    "# Extract dominant colors (first 3 colors, each with RGB values)\n",
    "n_products = len(full)\n",
    "colors_per_product = 3\n",
    "color_swatches = []\n",
    "\n",
    "for idx in range(min(10, n_products)):  # Show first 10 products\n",
    "    # Get RGB values for 3 dominant colors\n",
    "    r1, g1, b1 = full.iloc[idx]['color_feat_0'], full.iloc[idx]['color_feat_1'], full.iloc[idx]['color_feat_2']\n",
    "    r2, g2, b2 = full.iloc[idx]['color_feat_3'], full.iloc[idx]['color_feat_4'], full.iloc[idx]['color_feat_5']\n",
    "    r3, g3, b3 = full.iloc[idx]['color_feat_6'], full.iloc[idx]['color_feat_7'], full.iloc[idx]['color_feat_8']\n",
    "    \n",
    "    # Get coverage percentages\n",
    "    pct1 = full.iloc[idx]['color_feat_9']\n",
    "    pct2 = full.iloc[idx]['color_feat_10']\n",
    "    pct3 = full.iloc[idx]['color_feat_11']\n",
    "    \n",
    "    # Normalize RGB to 0-1\n",
    "    colors = [\n",
    "        (r1/255, g1/255, b1/255),\n",
    "        (r2/255, g2/255, b2/255),\n",
    "        (r3/255, g3/255, b3/255)\n",
    "    ]\n",
    "    \n",
    "    y_pos = idx * 0.8\n",
    "    x_start = 0\n",
    "    for i, (color, pct) in enumerate(zip(colors, [pct1, pct2, pct3])):\n",
    "        width = pct * 0.8\n",
    "        rect = Rectangle((x_start, y_pos), width, 0.6, facecolor=color, edgecolor='black', linewidth=0.5)\n",
    "        ax1.add_patch(rect)\n",
    "        x_start += width\n",
    "    \n",
    "    ax1.text(-0.05, y_pos + 0.3, f'P{idx+1}', ha='right', va='center', fontsize=8)\n",
    "\n",
    "ax1.set_xlim(-0.2, 0.85)\n",
    "ax1.set_ylim(-0.2, min(10, n_products) * 0.8)\n",
    "ax1.set_xlabel('Color Coverage (Proportional)')\n",
    "ax1.set_ylabel('Product Index')\n",
    "ax1.set_title('Dominant Color Palettes (Top 3 Colors per Product)')\n",
    "ax1.set_yticks([i*0.8 + 0.3 for i in range(min(10, n_products))])\n",
    "ax1.set_yticklabels([f'P{i+1}' for i in range(min(10, n_products))])\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# 2.2 Hue Histogram Comparison\n",
    "ax2 = axes[0, 1]\n",
    "hue_bins = 12\n",
    "hue_data = np.array([full[f'color_feat_{i+12}'].values for i in range(hue_bins)])\n",
    "x_pos_hue = np.arange(hue_bins)\n",
    "width_hue = 0.8 / n_products\n",
    "\n",
    "for idx in range(n_products):\n",
    "    offset = (idx - n_products/2) * width_hue\n",
    "    ax2.bar(x_pos_hue + offset, hue_data[:, idx], width_hue, alpha=0.7, \n",
    "            label=f'P{idx+1}' if idx < 5 else '')\n",
    "\n",
    "ax2.set_xlabel('Hue Bin (0-360°)')\n",
    "ax2.set_ylabel('Normalized Frequency')\n",
    "ax2.set_title('Hue Distribution Across Products')\n",
    "ax2.set_xticks(x_pos_hue)\n",
    "ax2.set_xticklabels([f'{i*30}°' for i in range(hue_bins)], rotation=45)\n",
    "ax2.legend(ncol=2, fontsize=7)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2.3 Average Dominant Color Coverage\n",
    "ax3 = axes[1, 0]\n",
    "coverage_data = [full['color_feat_9'].mean(), full['color_feat_10'].mean(), full['color_feat_11'].mean()]\n",
    "colors_avg = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "bars = ax3.bar(['Color 1', 'Color 2', 'Color 3'], coverage_data, color=colors_avg, alpha=0.8, edgecolor='black')\n",
    "ax3.set_ylabel('Average Coverage Percentage')\n",
    "ax3.set_title('Average Coverage of Dominant Colors')\n",
    "ax3.set_ylim(0, max(coverage_data) * 1.2)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, coverage_data):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2.4 Color Diversity (Hue Histogram Entropy)\n",
    "ax4 = axes[1, 1]\n",
    "from scipy.stats import entropy\n",
    "\n",
    "hue_entropies = []\n",
    "for idx in range(n_products):\n",
    "    hue_hist = [full.iloc[idx][f'color_feat_{i+12}'] for i in range(hue_bins)]\n",
    "    # Normalize to probability distribution\n",
    "    hue_hist = np.array(hue_hist)\n",
    "    hue_hist = hue_hist / hue_hist.sum() if hue_hist.sum() > 0 else hue_hist\n",
    "    ent = entropy(hue_hist + 1e-10)  # Add small value to avoid log(0)\n",
    "    hue_entropies.append(ent)\n",
    "\n",
    "ax4.bar(range(n_products), hue_entropies, color='#95E1D3', alpha=0.8, edgecolor='black')\n",
    "ax4.set_xlabel('Product Index')\n",
    "ax4.set_ylabel('Hue Entropy (Color Diversity)')\n",
    "ax4.set_title('Color Diversity Score (Higher = More Diverse)')\n",
    "ax4.set_xticks(range(n_products))\n",
    "ax4.set_xticklabels([f'P{i+1}' for i in range(n_products)], rotation=45, ha='right')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '02_color_features.png', bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. TEXTURE & GRAPHICS FEATURE VISUALIZATION\n",
    "# ============================================================================\n",
    "print(\"\\n3. Creating Texture & Graphics Feature Charts...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Texture & Graphics Feature Analysis', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 3.1 Edge Density Distribution\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(full['edge_density'], bins=15, color='#FF6B6B', alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(full['edge_density'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {full[\"edge_density\"].mean():.3f}')\n",
    "ax1.set_xlabel('Edge Density (0-1)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Edge Density Distribution\\n(Higher = More Complex/Busy Design)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 3.2 HOG Mean vs Edge Density\n",
    "ax2 = axes[0, 1]\n",
    "scatter = ax2.scatter(full['hog_mean'], full['edge_density'], \n",
    "                     c=full['Sale'] if 'Sale' in full.columns else range(len(full)),\n",
    "                     s=150, alpha=0.7, cmap='viridis', edgecolors='black', linewidth=0.5)\n",
    "ax2.set_xlabel('HOG Mean (Shape/Edge Patterns)')\n",
    "ax2.set_ylabel('Edge Density')\n",
    "ax2.set_title('HOG vs Edge Density Relationship')\n",
    "if 'Sale' in full.columns:\n",
    "    cbar = plt.colorbar(scatter, ax=ax2)\n",
    "    cbar.set_label('Sales', rotation=270, labelpad=15)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3.3 LBP Histogram (Average across all products)\n",
    "ax3 = axes[1, 0]\n",
    "lbp_cols = [f'lbp_{i}' for i in range(16)]\n",
    "lbp_avg = [full[col].mean() for col in lbp_cols]\n",
    "bars = ax3.bar(range(16), lbp_avg, color='#4ECDC4', alpha=0.8, edgecolor='black')\n",
    "ax3.set_xlabel('LBP Pattern Bin')\n",
    "ax3.set_ylabel('Average Normalized Frequency')\n",
    "ax3.set_title('Average Local Binary Pattern (LBP) Histogram\\n(Texture Pattern Distribution)')\n",
    "ax3.set_xticks(range(16))\n",
    "ax3.set_xticklabels([f'B{i}' for i in range(16)], rotation=45, ha='right')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3.4 Texture Complexity Score (Combined)\n",
    "ax4 = axes[1, 1]\n",
    "# Create a combined texture complexity score\n",
    "texture_complexity = (full['edge_density'] * 0.4 + \n",
    "                     (full['hog_mean'] / full['hog_mean'].max()) * 0.3 +\n",
    "                     (full[[f'lbp_{i}' for i in range(16)]].sum(axis=1) / \n",
    "                      full[[f'lbp_{i}' for i in range(16)]].sum(axis=1).max()) * 0.3)\n",
    "\n",
    "ax4.bar(range(n_products), texture_complexity, color='#95E1D3', alpha=0.8, edgecolor='black')\n",
    "ax4.set_xlabel('Product Index')\n",
    "ax4.set_ylabel('Texture Complexity Score')\n",
    "ax4.set_title('Combined Texture Complexity\\n(Edge Density + HOG + LBP)')\n",
    "ax4.set_xticks(range(n_products))\n",
    "ax4.set_xticklabels([f'P{i+1}' for i in range(n_products)], rotation=45, ha='right')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '03_texture_features.png', bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 4. LAYOUT & LOGO FEATURE VISUALIZATION\n",
    "# ============================================================================\n",
    "print(\"\\n4. Creating Layout & Logo Feature Charts...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Layout & Logo Feature Analysis', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 4.1 Aspect Ratio Distribution\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(full['aspect_ratio'], bins=15, color='#FF6B6B', alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(1.0, color='red', linestyle='--', linewidth=2, label='Square (1.0)')\n",
    "ax1.axvline(full['aspect_ratio'].mean(), color='blue', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: {full[\"aspect_ratio\"].mean():.3f}')\n",
    "ax1.set_xlabel('Aspect Ratio (Width/Height)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Aspect Ratio Distribution')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 4.2 White Space Percentage\n",
    "ax2 = axes[0, 1]\n",
    "ax2.bar(range(n_products), full['white_pct'], color='#4ECDC4', alpha=0.8, edgecolor='black')\n",
    "ax2.set_xlabel('Product Index')\n",
    "ax2.set_ylabel('White Space Percentage')\n",
    "ax2.set_title('White Space Usage\\n(Higher = More Minimalist Design)')\n",
    "ax2.set_xticks(range(n_products))\n",
    "ax2.set_xticklabels([f'P{i+1}' for i in range(n_products)], rotation=45, ha='right')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4.3 Logo Score Distribution\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(full['logo_score'], bins=15, color='#45B7D1', alpha=0.7, edgecolor='black')\n",
    "ax3.axvline(full['logo_score'].mean(), color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: {full[\"logo_score\"].mean():.4f}')\n",
    "ax3.set_xlabel('Logo Score')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title('Logo Prominence Score Distribution\\n(Higher = More Prominent Branding)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4.4 Layout Design Space (White Space vs Logo Score)\n",
    "ax4 = axes[1, 1]\n",
    "scatter = ax4.scatter(full['white_pct'], full['logo_score'],\n",
    "                     c=full['Sale'] if 'Sale' in full.columns else range(len(full)),\n",
    "                     s=150, alpha=0.7, cmap='plasma', edgecolors='black', linewidth=0.5)\n",
    "ax4.set_xlabel('White Space Percentage')\n",
    "ax4.set_ylabel('Logo Score')\n",
    "ax4.set_title('Design Space: White Space vs Logo Prominence')\n",
    "if 'Sale' in full.columns:\n",
    "    cbar = plt.colorbar(scatter, ax=ax4)\n",
    "    cbar.set_label('Sales', rotation=270, labelpad=15)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '04_layout_logo_features.png', bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 5. TYPOGRAPHY FEATURE VISUALIZATION\n",
    "# ============================================================================\n",
    "print(\"\\n5. Creating Typography Feature Charts...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Typography Feature Analysis', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 5.1 Text Percentage Distribution\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(full['text_pct'], bins=15, color='#FF6B6B', alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(full['text_pct'].mean(), color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: {full[\"text_pct\"].mean():.3f}')\n",
    "ax1.set_xlabel('Text Percentage (0-1)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Text Coverage Distribution\\n(Higher = More Information-Dense)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 5.2 Text Count Distribution\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(full['text_cnts'], bins=15, color='#4ECDC4', alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(full['text_cnts'].mean(), color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: {full[\"text_cnts\"].mean():.1f}')\n",
    "ax2.set_xlabel('Number of Text Regions')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Text Region Count Distribution\\n(Higher = More Text Blocks)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 5.3 Text Percentage vs Text Count\n",
    "ax3 = axes[1, 0]\n",
    "scatter = ax3.scatter(full['text_pct'], full['text_cnts'],\n",
    "                     c=full['Sale'] if 'Sale' in full.columns else range(len(full)),\n",
    "                     s=150, alpha=0.7, cmap='coolwarm', edgecolors='black', linewidth=0.5)\n",
    "ax3.set_xlabel('Text Percentage')\n",
    "ax3.set_ylabel('Text Region Count')\n",
    "ax3.set_title('Typography Relationship: Coverage vs Complexity')\n",
    "if 'Sale' in full.columns:\n",
    "    cbar = plt.colorbar(scatter, ax=ax3)\n",
    "    cbar.set_label('Sales', rotation=270, labelpad=15)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 5.4 Typography Complexity Score (Combined)\n",
    "ax4 = axes[1, 1]\n",
    "# Normalize and combine text features\n",
    "text_pct_norm = (full['text_pct'] - full['text_pct'].min()) / (full['text_pct'].max() - full['text_pct'].min() + 1e-10)\n",
    "text_cnts_norm = (full['text_cnts'] - full['text_cnts'].min()) / (full['text_cnts'].max() - full['text_cnts'].min() + 1e-10)\n",
    "typography_complexity = (text_pct_norm * 0.6 + text_cnts_norm * 0.4)\n",
    "\n",
    "ax4.bar(range(n_products), typography_complexity, color='#95E1D3', alpha=0.8, edgecolor='black')\n",
    "ax4.set_xlabel('Product Index')\n",
    "ax4.set_ylabel('Typography Complexity Score')\n",
    "ax4.set_title('Combined Typography Complexity\\n(Text Coverage + Text Blocks)')\n",
    "ax4.set_xticks(range(n_products))\n",
    "ax4.set_xticklabels([f'P{i+1}' for i in range(n_products)], rotation=45, ha='right')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '05_typography_features.png', bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 6. COMPREHENSIVE FEATURE SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n6. Creating Comprehensive Feature Summary...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Comprehensive Feature Extraction Summary', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 6.1 Feature Categories Overview\n",
    "ax1 = axes[0, 0]\n",
    "feature_categories = {\n",
    "    'Basic Stats': 6,  # mean_r, mean_g, mean_b, std_r, std_g, std_b\n",
    "    'Color': 24,       # color_feat_0 to color_feat_23\n",
    "    'Texture': 18,     # hog_mean, edge_density, lbp_0 to lbp_15\n",
    "    'Layout/Logo': 3,  # aspect_ratio, white_pct, logo_score\n",
    "    'Typography': 2,  # text_pct, text_cnts\n",
    "    'Embeddings': 64   # emb_0 to emb_63\n",
    "}\n",
    "colors_cat = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#95E1D3', '#F38181', '#AA96DA']\n",
    "bars = ax1.bar(feature_categories.keys(), feature_categories.values(), \n",
    "               color=colors_cat[:len(feature_categories)], alpha=0.8, edgecolor='black')\n",
    "ax1.set_ylabel('Number of Features')\n",
    "ax1.set_title('Feature Count by Category')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "for bar, val in zip(bars, feature_categories.values()):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{val}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 6.2 Feature Value Ranges (Normalized)\n",
    "ax2 = axes[0, 1]\n",
    "# Sample key features to show ranges\n",
    "key_features = {\n",
    "    'mean_r': full['mean_r'].mean(),\n",
    "    'edge_density': full['edge_density'].mean() * 255,  # Scale for comparison\n",
    "    'white_pct': full['white_pct'].mean() * 255,\n",
    "    'text_pct': full['text_pct'].mean() * 255,\n",
    "    'logo_score': full['logo_score'].mean() * 10000  # Scale for visibility\n",
    "}\n",
    "bars = ax2.bar(key_features.keys(), key_features.values(), \n",
    "               color='#95E1D3', alpha=0.8, edgecolor='black')\n",
    "ax2.set_ylabel('Average Value (Normalized)')\n",
    "ax2.set_title('Key Feature Average Values')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 6.3 Feature Correlation Heatmap (Sample)\n",
    "ax3 = axes[0, 2]\n",
    "sample_features = ['mean_r', 'mean_g', 'mean_b', 'edge_density', 'white_pct', \n",
    "                   'logo_score', 'text_pct', 'text_cnts', 'hog_mean']\n",
    "corr_data = full[sample_features].corr()\n",
    "im = ax3.imshow(corr_data, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "ax3.set_xticks(range(len(sample_features)))\n",
    "ax3.set_yticks(range(len(sample_features)))\n",
    "ax3.set_xticklabels(sample_features, rotation=45, ha='right')\n",
    "ax3.set_yticklabels(sample_features)\n",
    "ax3.set_title('Feature Correlation Matrix')\n",
    "plt.colorbar(im, ax=ax3, label='Correlation')\n",
    "\n",
    "# 6.4 Feature Distribution Comparison (Box Plot)\n",
    "ax4 = axes[1, 0]\n",
    "sample_data = [full['mean_r'], full['edge_density']*255, full['white_pct']*255, \n",
    "               full['text_pct']*255]\n",
    "bp = ax4.boxplot(sample_data, labels=['RGB Mean', 'Edge Density', 'White %', 'Text %'],\n",
    "                 patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], ['#FF6B6B', '#4ECDC4', '#45B7D1', '#95E1D3']):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "ax4.set_ylabel('Normalized Value')\n",
    "ax4.set_title('Feature Distribution Comparison')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 6.5 Feature Importance by Category (if Sale column exists)\n",
    "ax5 = axes[1, 1]\n",
    "if 'Sale' in full.columns:\n",
    "    # Calculate correlation with sales for each category\n",
    "    category_corrs = {\n",
    "        'Basic Stats': abs(full[['mean_r', 'mean_g', 'mean_b']].corrwith(full['Sale'])).mean(),\n",
    "        'Color': abs(full[[f'color_feat_{i}' for i in range(24)]].corrwith(full['Sale'])).mean(),\n",
    "        'Texture': abs(full[['edge_density', 'hog_mean'] + [f'lbp_{i}' for i in range(16)]].corrwith(full['Sale'])).mean(),\n",
    "        'Layout': abs(full[['aspect_ratio', 'white_pct', 'logo_score']].corrwith(full['Sale'])).mean(),\n",
    "        'Typography': abs(full[['text_pct', 'text_cnts']].corrwith(full['Sale'])).mean()\n",
    "    }\n",
    "    bars = ax5.bar(category_corrs.keys(), category_corrs.values(), \n",
    "                   color=colors_cat[:len(category_corrs)], alpha=0.8, edgecolor='black')\n",
    "    ax5.set_ylabel('Average |Correlation| with Sales')\n",
    "    ax5.set_title('Feature Category Importance (Sales Correlation)')\n",
    "    ax5.tick_params(axis='x', rotation=45)\n",
    "    ax5.grid(True, alpha=0.3, axis='y')\n",
    "    for bar, val in zip(bars, category_corrs.values()):\n",
    "        height = bar.get_height()\n",
    "        ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                 f'{val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "else:\n",
    "    ax5.text(0.5, 0.5, 'Sales data not available\\nfor correlation analysis', \n",
    "            ha='center', va='center', transform=ax5.transAxes, fontsize=12)\n",
    "    ax5.set_title('Feature Category Importance')\n",
    "\n",
    "# 6.6 Feature Extraction Pipeline Summary\n",
    "ax6 = axes[1, 2]\n",
    "ax6.axis('off')\n",
    "pipeline_text = \"\"\"\n",
    "FEATURE EXTRACTION PIPELINE\n",
    "\n",
    "1. Basic Image Statistics\n",
    "   → RGB Mean & Std (6 features)\n",
    "\n",
    "2. Color Features\n",
    "   → Dominant Colors (K-Means)\n",
    "   → Color Coverage\n",
    "   → Hue Histogram (24 features)\n",
    "\n",
    "3. Texture & Graphics\n",
    "   → HOG (Histogram of Oriented Gradients)\n",
    "   → Edge Density (Sobel)\n",
    "   → LBP (Local Binary Pattern) (18 features)\n",
    "\n",
    "4. Layout & Logo\n",
    "   → Aspect Ratio\n",
    "   → White Space %\n",
    "   → Logo Score (3 features)\n",
    "\n",
    "5. Typography\n",
    "   → Text Coverage %\n",
    "   → Text Region Count (2 features)\n",
    "\n",
    "6. Deep Learning\n",
    "   → ResNet50 Embeddings (64 features)\n",
    "\n",
    "TOTAL: ~117 Features per Image\n",
    "\"\"\"\n",
    "ax6.text(0.1, 0.5, pipeline_text, transform=ax6.transAxes,\n",
    "         fontsize=10, verticalalignment='center', family='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '06_comprehensive_summary.png', bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All visualizations saved to:\", OUTPUT_DIR)\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated Charts:\")\n",
    "print(\"  1. 01_basic_image_statistics.png - RGB channel analysis\")\n",
    "print(\"  2. 02_color_features.png - Color palette and hue distribution\")\n",
    "print(\"  3. 03_texture_features.png - Texture and graphics analysis\")\n",
    "print(\"  4. 04_layout_logo_features.png - Layout and logo analysis\")\n",
    "print(\"  5. 05_typography_features.png - Typography analysis\")\n",
    "print(\"  6. 06_comprehensive_summary.png - Overall feature summary\")\n",
    "print(\"=\"*80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5921542f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mint\u001b[39m(text) \u001b[38;5;28;01mif\u001b[39;00m text.isdigit() \u001b[38;5;28;01melse\u001b[39;00m text.lower() \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m re.split(\u001b[33m'\u001b[39m\u001b[33m([0-9]+)\u001b[39m\u001b[33m'\u001b[39m, s)]\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m img_files = \u001b[38;5;28msorted\u001b[39m([f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m.listdir(images_dir) \u001b[38;5;28;01mif\u001b[39;00m f.lower().endswith((\u001b[33m'\u001b[39m\u001b[33m.png\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m.jpg\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m.jpeg\u001b[39m\u001b[33m'\u001b[39m))], key=natural_key)\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(img_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m image files in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimages_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, sample:\u001b[39m\u001b[33m\"\u001b[39m, img_files[:\u001b[32m6\u001b[39m])\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# SECTION 3: IMAGE LOADING FUNCTION\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "### 2. Load and Inspect Data\n",
    "# Ensure Path is imported (in case this cell runs before Cell 0)\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root(start_path=None):\n",
    "    \"\"\"Find the MainProject directory by walking up from start_path\"\"\"\n",
    "    if start_path is None:\n",
    "        start_path = Path.cwd()\n",
    "    else:\n",
    "        start_path = Path(start_path).resolve()\n",
    "    \n",
    "    current = start_path\n",
    "    while current != current.parent:\n",
    "        # Check if this directory is named \"MainProject\"\n",
    "        if current.name == \"MainProject\":\n",
    "            return current\n",
    "        current = current.parent\n",
    "    \n",
    "    # If not found, return the original start_path as fallback\n",
    "    return start_path\n",
    "\n",
    "# Get the MainProject root directory\n",
    "PROJECT_DIR = find_project_root()\n",
    "excel_path = PROJECT_DIR / \"ProteinProducts.xlsx\"\n",
    "images_dir = PROJECT_DIR / \"ProteinProductImages\"\n",
    "OUTPUT_DIR = PROJECT_DIR / \"ml_outputs\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "CV_FOLDS = 3\n",
    "PCA_MAX_COMPONENTS = 8\n",
    "\n",
    "\n",
    "\n",
    "# List images and sort to ensure ordering (1→row1, 2→row2 ...)\n",
    "# Sorting strategy: natural sort on filenames; if filenames are numbers or prefixed, adjust accordingly.\n",
    "def natural_key(s):\n",
    "    import re\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]\n",
    "img_files = sorted([f for f in os.listdir(images_dir) if f.lower().endswith(('.png','.jpg','.jpeg'))], key=natural_key)\n",
    "print(f\"Found {len(img_files)} image files in {images_dir}, sample:\", img_files[:6])\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: IMAGE LOADING FUNCTION\n",
    "# ============================================================================\n",
    "#\n",
    "# PURPOSE:\n",
    "#   - Create a reusable function to load images from file paths\n",
    "#   - Convert images to numpy arrays for processing\n",
    "#   - Handle different image formats consistently\n",
    "#\n",
    "# WHAT WE'RE DOING:\n",
    "#   - Define load_image() function that:\n",
    "#     1. Opens image file using PIL\n",
    "#     2. Converts to RGB (handles grayscale, RGBA, etc.)\n",
    "#     3. Optionally resizes to target_size\n",
    "#     4. Returns as numpy array (H x W x 3, values 0-255)\n",
    "#\n",
    "# EXPECTED RESULT:\n",
    "#   - Function ready to use for loading images\n",
    "#   - No output (just function definition)\n",
    "#\n",
    "# ============================================================================\n",
    "df = pd.read_excel(excel_path, sheet_name=0)\n",
    "df = df.copy()\n",
    "print(\"Excel shape:\", df.shape)\n",
    "display(df.head())\n",
    "\n",
    "# Ensure counts match (images ↔ rows)\n",
    "if len(img_files) != len(df):\n",
    "    print(\"WARNING: number of images != number of rows. Mapping will use the first N images to the first N rows.\")\n",
    "n = min(len(img_files), len(df))\n",
    "df = df.iloc[:n].reset_index(drop=True)\n",
    "\n",
    "# Add image path column (1→first row, 2→second row...)\n",
    "df['image_path'] = [os.path.join(images_dir, f) for f in img_files[:len(df)]]\n",
    "print(df[['Product Name','image_path']].head())\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: BASIC IMAGE STATISTICS EXTRACTION\n",
    "# ============================================================================\n",
    "#\n",
    "# PURPOSE:\n",
    "#   - Extract fundamental color statistics from images\n",
    "#   - These are simple but informative features about overall image appearance\n",
    "#\n",
    "# WHAT WE'RE DOING:\n",
    "#   - Define image_stats() function that calculates:\n",
    "#     1. Mean RGB values (average color per channel)\n",
    "#     2. Standard deviation RGB values (color variation/contrast)\n",
    "#\n",
    "# FEATURES EXTRACTED:\n",
    "#   - mean_r, mean_g, mean_b: Average red, green, blue intensity (0-255)\n",
    "#   - std_r, std_g, std_b: Color variation (higher = more contrast)\n",
    "#\n",
    "# INTERPRETATION:\n",
    "#   - High mean values = bright image\n",
    "#   - Low std values = uniform/flat colors\n",
    "#   - High std values = high contrast/variety of colors\n",
    "#\n",
    "# EXPECTED RESULT:\n",
    "#   - Function returns 2 arrays: (mean, std) each with 3 values (R, G, B)\n",
    "#\n",
    "# ============================================================================\n",
    "def load_image(path, target_size=None):\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    if target_size:\n",
    "        img = img.resize(target_size, Image.BICUBIC)\n",
    "    return np.array(img)\n",
    "\n",
    "def image_stats(img_arr):\n",
    "    # img_arr: numpy array HxWx3 (0-255)\n",
    "    img = Image.fromarray(img_arr.astype('uint8'))\n",
    "    stat = ImageStat.Stat(img)\n",
    "    mean = stat.mean  # per channel\n",
    "    std = stat.stddev\n",
    "    return mean, std\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: COLOR FEATURE EXTRACTION\n",
    "# ============================================================================\n",
    "#\n",
    "# PURPOSE:\n",
    "#   - Identify dominant colors in product images\n",
    "#   - Understand color palette and distribution\n",
    "#   - Extract color histogram information\n",
    "#\n",
    "# WHAT WE'RE DOING:\n",
    "#   - Define color_features() function that:\n",
    "#     1. Uses K-Means clustering to find k dominant colors (default k=3)\n",
    "#     2. Calculates percentage of image covered by each dominant color\n",
    "#     3. Creates hue histogram (12 bins) to capture color distribution\n",
    "#\n",
    "# FEATURES EXTRACTED:\n",
    "#   - color_feat_0 to color_feat_8: RGB values of 3 dominant colors (9 values)\n",
    "#   - color_feat_9 to color_feat_11: Percentage coverage of each dominant color (3 values)\n",
    "#   - color_feat_12 to color_feat_23: Hue histogram (12 values)\n",
    "#   - Total: 24 color features per image\n",
    "#\n",
    "# INTERPRETATION:\n",
    "#   - Dominant colors reveal brand identity and visual appeal\n",
    "#   - Hue histogram shows color diversity (peaks = common colors)\n",
    "#   - Products with similar color palettes may have similar sales patterns\n",
    "#\n",
    "# EXPECTED RESULT:\n",
    "#   - Function returns list of 24 color-related features\n",
    "#\n",
    "# ============================================================================\n",
    "from sklearn.cluster import KMeans\n",
    "def color_features(img_arr, k=3):\n",
    "    # returns: dominant color RGBs flattened, percent of each, color histogram (HSV)\n",
    "    h, w, _ = img_arr.shape\n",
    "    pixels = img_arr.reshape(-1,3).astype(float)\n",
    "\n",
    "    # small subsample for speed\n",
    "    sample_idx = np.random.choice(len(pixels), size=min(5000, len(pixels)), replace=False)\n",
    "    sample = pixels[sample_idx]\n",
    "    km = KMeans(n_clusters=k, random_state=42).fit(sample)\n",
    "    centers = km.cluster_centers_.astype(int)\n",
    "    labels_full = KMeans(n_clusters=k, random_state=42).fit(sample).labels_  # note: cheap approximation\n",
    "\n",
    "    # percent: approximate via nearest center on the sample\n",
    "    counts = np.bincount(labels_full, minlength=k) / len(labels_full)\n",
    "    # color histogram in HSV\n",
    "    from matplotlib.colors import rgb_to_hsv\n",
    "    hsv = rgb_to_hsv(img_arr/255.0)\n",
    "    hvals = (hsv[:,:,0].ravel() * 360)\n",
    "    # histogram for hue\n",
    "    hue_hist, _ = np.histogram(hvals, bins=12, range=(0,360), density=True)\n",
    "    return centers.flatten().tolist() + counts.tolist() + hue_hist.tolist()\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: TEXTURE & GRAPHICS FEATURE EXTRACTION\n",
    "# ============================================================================\n",
    "#\n",
    "# PURPOSE:\n",
    "#   - Capture visual texture and graphic design elements\n",
    "#   - Measure image complexity and visual patterns\n",
    "#\n",
    "# WHAT WE'RE DOING:\n",
    "#   - Define texture_features() function that extracts:\n",
    "#     1. HOG (Histogram of Oriented Gradients): Captures edge patterns and shapes\n",
    "#     2. Edge density: Percentage of image with strong edges (using Sobel filter)\n",
    "#     3. LBP (Local Binary Pattern): Texture descriptor (16-bin histogram)\n",
    "#\n",
    "# FEATURES EXTRACTED:\n",
    "#   - hog_mean: Average HOG feature value (captures shape/edge patterns)\n",
    "#   - edge_density: Fraction of pixels with strong edges (0-1)\n",
    "#   - lbp_0 to lbp_15: Texture pattern histogram (16 values)\n",
    "#\n",
    "# INTERPRETATION:\n",
    "#   - High edge_density = busy/complex design, lots of text/graphics\n",
    "#   - HOG captures overall shape patterns (logos, text blocks, etc.)\n",
    "#   - LBP captures fine texture details (smooth vs. textured surfaces)\n",
    "#\n",
    "# EXPECTED RESULT:\n",
    "#   - Function returns dictionary with: {'hog_mean': float, 'edge_density': float, 'lbp_hist': array}\n",
    "#\n",
    "# ============================================================================\n",
    "from skimage.feature import local_binary_pattern\n",
    "def texture_features(img_arr):\n",
    "    gray = rgb2gray(img_arr)\n",
    "    # HOG\n",
    "    hog_feat, hog_img = hog(gray, pixels_per_cell=(16,16), cells_per_block=(1,1), visualize=True, feature_vector=True)\n",
    "    # edge density\n",
    "    edges = sobel(gray)\n",
    "    edge_density = (edges > 0.02).mean()\n",
    "    # LBP histogram\n",
    "    lbp = local_binary_pattern(gray, P=8, R=1.0)\n",
    "    (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, 2**8 + 1), density=True)\n",
    "    return {\n",
    "        'hog_len': len(hog_feat),\n",
    "        'hog_mean': np.mean(hog_feat),\n",
    "        'edge_density': edge_density,\n",
    "        'lbp_hist': hist[:16].tolist()  # take first 16 bins to keep dims reasonable\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: LAYOUT & LOGO FEATURE EXTRACTION\n",
    "# ============================================================================\n",
    "#\n",
    "# PURPOSE:\n",
    "#   - Analyze product packaging layout and design structure\n",
    "#   - Identify logo presence and white space usage\n",
    "#\n",
    "# WHAT WE'RE DOING:\n",
    "#   - Define layout_logo_features() function that calculates:\n",
    "#     1. Aspect ratio: Width/Height (usually 1.0 for square images)\n",
    "#     2. White percentage: Fraction of image that is white/light (background space)\n",
    "#     3. Logo score: Measure of logo-like regions (high contrast, compact areas)\n",
    "#\n",
    "# FEATURES EXTRACTED:\n",
    "#   - aspect_ratio: Image dimensions ratio (usually 1.0)\n",
    "#   - white_pct: Percentage of white/light pixels (0-1)\n",
    "#   - logo_score: Numerical score indicating logo prominence\n",
    "#\n",
    "# INTERPRETATION:\n",
    "#   - High white_pct = minimalist design, clean packaging\n",
    "#   - High logo_score = prominent branding/logo\n",
    "#   - Layout features affect visual appeal and brand recognition\n",
    "#\n",
    "# EXPECTED RESULT:\n",
    "#   - Function returns tuple: (aspect_ratio, white_pct, logo_score)\n",
    "#\n",
    "# ============================================================================\n",
    "import cv2\n",
    "def layout_logo_features(img_arr):\n",
    "    h, w, _ = img_arr.shape\n",
    "    aspect_ratio = w / h\n",
    "    # whitespace proxy: percent of near-white pixels\n",
    "    gray = cv2.cvtColor(img_arr.astype('uint8'), cv2.COLOR_RGB2GRAY)\n",
    "    white_pct = np.mean(gray > 245)\n",
    "    # logo proxy (simple): detect large connected components of high-contrast small objects using Canny\n",
    "    edges = cv2.Canny(gray, 100, 200)\n",
    "    logo_score = np.sum(edges) / (h*w)\n",
    "    return aspect_ratio, white_pct, logo_score\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 8: TYPOGRAPHY PROXY FEATURE EXTRACTION\n",
    "# ============================================================================\n",
    "#\n",
    "# PURPOSE:\n",
    "#   - Estimate text content and typography in product images\n",
    "#   - Measure how much text/information is displayed\n",
    "#\n",
    "# WHAT WE'RE DOING:\n",
    "#   - Define typography_proxy() function that:\n",
    "#     1. Converts image to grayscale\n",
    "#     2. Uses adaptive thresholding to detect text-like regions\n",
    "#     3. Counts text-like blobs (contours)\n",
    "#\n",
    "# FEATURES EXTRACTED:\n",
    "#   - text_pct: Percentage of image that appears to be text (0-1)\n",
    "#   - text_cnts: Number of distinct text regions/blobs\n",
    "#\n",
    "# INTERPRETATION:\n",
    "#   - High text_pct = information-dense packaging (nutrition facts, ingredients)\n",
    "#   - High text_cnts = multiple text blocks (complex information layout)\n",
    "#   - Low values = minimal text, image-focused design\n",
    "#\n",
    "# EXPECTED RESULT:\n",
    "#   - Function returns tuple: (text_pct, text_cnts)\n",
    "#\n",
    "# ============================================================================\n",
    "# We'll use a simple proxy: presence of text via high-contrast connected components using MSER-like approach\n",
    "def typography_proxy(img_arr):\n",
    "    gray = cv2.cvtColor(img_arr.astype('uint8'), cv2.COLOR_RGB2GRAY)\n",
    "    # adaptive threshold to find potential text regions\n",
    "    th = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV,11,2)\n",
    "    # percent of image that's text-like\n",
    "    text_pct = np.mean(th>0)\n",
    "    # count of contours (roughly text blobs)\n",
    "    contours, _ = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = len(contours)\n",
    "    return text_pct, cnts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------- 9. CNN embeddings (ResNet50) ----------\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 9: DEEP LEARNING EMBEDDINGS (RESNET50)\n",
    "# ============================================================================\n",
    "#\n",
    "# PURPOSE:\n",
    "#   - Extract high-level visual features using pre-trained deep learning model\n",
    "#   - Capture complex visual patterns that traditional features might miss\n",
    "#   - Use transfer learning from ImageNet (millions of images)\n",
    "#\n",
    "# WHAT WE'RE DOING:\n",
    "#   1. Load ResNet50 model pre-trained on ImageNet\n",
    "#      - include_top=False: Remove final classification layer\n",
    "#      - pooling='avg': Global average pooling (2048-dim vector)\n",
    "#   2. Define resnet_embed() function that:\n",
    "#      - Resizes image to 224x224 (ResNet50 input size)\n",
    "#      - Preprocesses using ResNet50's preprocessing\n",
    "#      - Extracts 2048-dimensional feature vector\n",
    "#\n",
    "# FEATURES EXTRACTED:\n",
    "#   - emb_0 to emb_63: First 64 dimensions of 2048-dim embedding\n",
    "#   - These capture high-level visual concepts (shapes, patterns, objects)\n",
    "#\n",
    "# INTERPRETATION:\n",
    "#   - Deep embeddings capture complex visual relationships\n",
    "#   - Similar products will have similar embedding values\n",
    "#   - Can identify visual similarities not obvious to human eye\n",
    "#\n",
    "# EXPECTED RESULT:\n",
    "#   - ResNet50 model loaded (may take time on first run to download weights)\n",
    "#   - Function returns 2048-dim array (we use first 64)\n",
    "#   - WARNING: May see SSL certificate warnings (handled in Section 0)\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "# We'll use global average pooled features from ResNet50 (pretrained on imagenet)\n",
    "resnet = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(224,224,3))\n",
    "\n",
    "def resnet_embed(img_arr):\n",
    "    # resize to 224x224\n",
    "    img = Image.fromarray(img_arr.astype('uint8')).resize((224,224))\n",
    "    x = np.array(img).astype('float32')\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    feat = resnet.predict(x, verbose=0)\n",
    "    return feat.flatten()\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 10: FEATURE TABLE CONSTRUCTION\n",
    "# ============================================================================\n",
    "#\n",
    "# PURPOSE:\n",
    "#   - Combine all extracted features into a single feature table\n",
    "#   - Create one row per product image with all computed features\n",
    "#\n",
    "# WHAT WE'RE DOING:\n",
    "#   For each product image:\n",
    "#   1. Load the image\n",
    "#   2. Extract all features (stats, color, texture, layout, typography, embeddings)\n",
    "#   3. Combine into single dictionary\n",
    "#   4. Append to feature_rows list\n",
    "#   5. Convert to DataFrame\n",
    "#\n",
    "# FEATURE BREAKDOWN (Total ~117 features):\n",
    "#   - Basic stats: 6 features (mean_r/g/b, std_r/g/b)\n",
    "#   - Color: 24 features (dominant colors, hue histogram)\n",
    "#   - Texture: 18 features (HOG, edge_density, 16 LBP bins)\n",
    "#   - Layout: 3 features (aspect_ratio, white_pct, logo_score)\n",
    "#   - Typography: 2 features (text_pct, text_cnts)\n",
    "#   - Embeddings: 64 features (first 64 dims of ResNet50)\n",
    "#\n",
    "# EXPECTED RESULT:\n",
    "#   - features_df: DataFrame with shape (20, 117)\n",
    "#   - Each row = one product, each column = one feature\n",
    "#   - Indexed by 'idx' (product index)\n",
    "#\n",
    "# WHAT TO CHECK:\n",
    "#   - All 20 products processed successfully\n",
    "#   - No missing values (NaN)\n",
    "#   - Feature values in expected ranges\n",
    "#\n",
    "# ============================================================================\n",
    "feature_rows = []\n",
    "for idx, row in df.iterrows():\n",
    "    p = row['image_path']\n",
    "    try:\n",
    "        img = load_image(p)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load\", p, e)\n",
    "        continue\n",
    "    # basic stats\n",
    "    mean, std = image_stats(img)\n",
    "    # color\n",
    "    color_feats = color_features(img, k=3)\n",
    "    # texture\n",
    "    tex = texture_features(img)\n",
    "    # layout/logo\n",
    "    aspect_ratio, white_pct, logo_score = layout_logo_features(img)\n",
    "    # typography proxy\n",
    "    text_pct, text_cnts = typography_proxy(img)\n",
    "    # cnn embedding (small dim e.g., 2048)\n",
    "    embed = resnet_embed(img)\n",
    "    # aggregate into dict\n",
    "    fr = {\n",
    "        'idx': idx,\n",
    "        'mean_r': mean[0], 'mean_g': mean[1], 'mean_b': mean[2],\n",
    "        'std_r': std[0], 'std_g': std[1], 'std_b': std[2],\n",
    "        'aspect_ratio': aspect_ratio,\n",
    "        'white_pct': white_pct,\n",
    "        'logo_score': logo_score,\n",
    "        'text_pct': text_pct,\n",
    "        'text_cnts': text_cnts,\n",
    "        'edge_density': tex['edge_density'],\n",
    "        'hog_mean': tex['hog_mean'],\n",
    "    }\n",
    "    # attach flattened color centers & counts & hue hist (3 centers x3 + 3 counts + 12 hue bins = 21 items)\n",
    "    for i, v in enumerate(color_feats):\n",
    "        fr[f'color_feat_{i}'] = float(v)\n",
    "    # attach first 16 lbp bins\n",
    "    for i, v in enumerate(tex['lbp_hist']):\n",
    "        fr[f'lbp_{i}'] = float(v)\n",
    "    # attach top 64 dims of embedding (or all but that may be heavy)\n",
    "    for i in range(64):\n",
    "        fr[f'emb_{i}'] = float(embed[i]) if i < len(embed) else 0.0\n",
    "    feature_rows.append(fr)\n",
    "\n",
    "\n",
    "\n",
    "features_df = pd.DataFrame(feature_rows).set_index('idx')\n",
    "print(\"Features shape:\", features_df.shape)\n",
    "display(features_df.head())\n",
    "\n",
    "# Merge features with original df\n",
    "full = pd.concat([df.reset_index(drop=True), features_df.reset_index()], axis=1)\n",
    "full = full.drop(columns=['index'], errors='ignore')\n",
    "print(\"Full merged shape:\", full.shape)\n",
    "display(full.head())\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 11: DATA MERGING & TARGET PREPARATION\n",
    "# ============================================================================\n",
    "#\n",
    "# PURPOSE:\n",
    "#   - Combine extracted features with original product data\n",
    "#   - Prepare target variables for both regression and classification tasks\n",
    "#\n",
    "# WHAT WE'RE DOING:\n",
    "#   1. Merge features_df with original df (product info)\n",
    "#   2. Convert 'Sale' to numeric (handle any text values)\n",
    "#   3. Create binary classification target 'HighSale':\n",
    "#      - HighSale = 1 if Sale > median(Sale)\n",
    "#      - HighSale = 0 if Sale <= median(Sale)\n",
    "#\n",
    "# EXPECTED RESULT:\n",
    "#   - full DataFrame: (20, 125) - original 8 cols + 117 features\n",
    "#   - 'Sale' column: numeric values (sales numbers)\n",
    "#   - 'HighSale' column: binary (0 or 1)\n",
    "#   - Print statements showing:\n",
    "#     * Sale median threshold: ~100,000 (example)\n",
    "#     * Distribution of Sale and HighSale values\n",
    "#\n",
    "# INTERPRETATION:\n",
    "#   - We can predict both:\n",
    "#     * Continuous sales (regression)\n",
    "#     * High vs. Low sales (classification)\n",
    "#\n",
    "# ============================================================================\n",
    "# We will use 'Sale' as numeric regression target.\n",
    "full['Sale'] = pd.to_numeric(full['Sale'], errors='coerce')\n",
    "# Create a binary target: HighSale = 1 if Sale > median (you can change threshold)\n",
    "threshold = full['Sale'].median()\n",
    "full['HighSale'] = (full['Sale'] > threshold).astype(int)\n",
    "print(\"Sale median threshold:\", threshold)\n",
    "display(full[['Sale','HighSale']].describe())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 12: FEATURE SELECTION & TRAIN-TEST SPLIT\n",
    "# ============================================================================\n",
    "#\n",
    "# PURPOSE:\n",
    "#   - Select relevant features for modeling\n",
    "#   - Split data into training and testing sets\n",
    "#   - Standardize features for linear models\n",
    "#\n",
    "# WHAT WE'RE DOING:\n",
    "#   1. Select features starting with: mean_, std_, color_feat_, lbp_, emb_, \n",
    "#      edge_density, hog_mean, aspect_ratio, white_pct, logo_score, text_pct, text_cnts\n",
    "#   2. Fill missing values with 0\n",
    "#   3. Standardize features (mean=0, std=1) using StandardScaler\n",
    "#   4. Split into train (80%) and test (20%)\n",
    "#\n",
    "# EXPECTED RESULT:\n",
    "#   - X: Feature matrix (~117 features)\n",
    "#   - y_reg: Continuous sales values\n",
    "#   - y_clf: Binary high/low sales (0 or 1)\n",
    "#   - X_train: (16, 117), X_test: (4, 117)\n",
    "#   - Print: \"Train/Test sizes: (16, 117) (4, 117)\"\n",
    "#\n",
    "# WHY STANDARDIZE?\n",
    "#   - Linear models (Ridge, Logistic) require standardized features\n",
    "#   - Tree models (RandomForest, XGBoost) don't need it but it doesn't hurt\n",
    "#\n",
    "# ============================================================================\n",
    "# choose a subset of features to keep things interpretable (means, color centers, hue hist, edge/texture, layout, typography proxies, embeds)\n",
    "keep_cols = [c for c in full.columns if c.startswith(('mean_','std_','color_feat_','lbp_','emb_','edge_density','hog_mean','aspect_ratio','white_pct','logo_score','text_pct','text_cnts'))]\n",
    "X = full[keep_cols].fillna(0)\n",
    "y_reg = full['Sale'].values\n",
    "y_clf = full['HighSale'].values\n",
    "\n",
    "# Standardize numeric features for linear models\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "X_train, X_test, y_reg_train, y_reg_test, y_clf_train, y_clf_test = train_test_split(\n",
    "    X_scaled, y_reg, y_clf, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"Train/Test sizes:\", X_train.shape, X_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 13: REGRESSION MODELS (PREDICTING SALES)\n",
    "# ============================================================================\n",
    "#\n",
    "# PURPOSE:\n",
    "#   - Train multiple models to predict continuous sales values\n",
    "#   - Compare model performance\n",
    "#   - Understand which models work best\n",
    "#\n",
    "# WHAT WE'RE DOING:\n",
    "#   Training 3 regression models:\n",
    "#   1. Ridge Regression: Linear model with L2 regularization\n",
    "#   2. Random Forest: Ensemble of decision trees\n",
    "#   3. XGBoost: Gradient boosting (advanced ensemble)\n",
    "#\n",
    "# EVALUATION METRICS:\n",
    "#   - RMSE (Root Mean Squared Error): Lower is better (in same units as sales)\n",
    "#   - R² (R-squared): Higher is better (0-1, % variance explained)\n",
    "#\n",
    "# EXPECTED RESULT:\n",
    "#   Output for each model:\n",
    "#   --- Ridge ---\n",
    "#   Train RMSE: [value]\n",
    "#   Test RMSE: [value]\n",
    "#   Test R2: [value]\n",
    "#\n",
    "#   --- RandomForest ---\n",
    "#   Train RMSE: [value]\n",
    "#   Test RMSE: [value]\n",
    "#   Test R2: [value]\n",
    "#\n",
    "#   --- XGBoost ---\n",
    "#   Train RMSE: [value]\n",
    "#   Test RMSE: [value]\n",
    "#   Test R2: [value]\n",
    "#\n",
    "# INTERPRETATION:\n",
    "#   - Lower Test RMSE = better predictions\n",
    "#   - R² close to 1 = model explains most variance\n",
    "#   - Large gap between Train/Test RMSE = overfitting\n",
    "#\n",
    "# ============================================================================\n",
    "# We'll fit: Ridge (linear), RandomForestRegressor, XGBoostRegressor (if available)\n",
    "ridge = Ridge(alpha=1.0)\n",
    "rf_reg = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "xgb_reg = xgb.XGBRegressor(n_estimators=200, random_state=42, verbosity=0)\n",
    "\n",
    "ridge.fit(X_train, y_reg_train)\n",
    "rf_reg.fit(X_train, y_reg_train)\n",
    "xgb_reg.fit(X_train, y_reg_train)\n",
    "\n",
    "def eval_reg(model, X_tr, X_te, y_tr, y_te, name='model'):\n",
    "    ypred_tr = model.predict(X_tr)\n",
    "    ypred_te = model.predict(X_te)\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(\"Train RMSE:\", math.sqrt(mean_squared_error(y_tr, ypred_tr)))\n",
    "    print(\"Test  RMSE:\", math.sqrt(mean_squared_error(y_te, ypred_te)))\n",
    "    print(\"Test R2:\", r2_score(y_te, ypred_te))\n",
    "    return ypred_te\n",
    "\n",
    "_ = eval_reg(ridge, X_train, X_test, y_reg_train, y_reg_test, \"Ridge\")\n",
    "_ = eval_reg(rf_reg, X_train, X_test, y_reg_train, y_reg_test, \"RandomForest\")\n",
    "_ = eval_reg(xgb_reg, X_train, X_test, y_reg_train, y_reg_test, \"XGBoost\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 14: FEATURE IMPORTANCE ANALYSIS (REGRESSION)\n",
    "# ============================================================================\n",
    "#\n",
    "# PURPOSE:\n",
    "#   - Identify which visual features most influence sales\n",
    "#   - Understand what makes products sell better\n",
    "#   - Guide product design decisions\n",
    "#\n",
    "# WHAT WE'RE DOING:\n",
    "#   1. Linear Model Coefficients: Which features have strongest linear relationship\n",
    "#   2. Random Forest Importance: Tree-based feature importance\n",
    "#   3. Permutation Importance: Robust measure (shuffles features, measures impact)\n",
    "#   4. SHAP Values: Explain individual predictions and global importance\n",
    "#\n",
    "# EXPECTED RESULT:\n",
    "#   1. coef_df: Top 20 features by absolute coefficient value\n",
    "#      - Positive coef = higher feature → higher sales\n",
    "#      - Negative coef = higher feature → lower sales\n",
    "#\n",
    "#   2. rf_imp: Top 20 features by Random Forest importance\n",
    "#      - Higher value = feature more important for predictions\n",
    "#\n",
    "#   3. perm_df: Top 20 features by permutation importance\n",
    "#      - Most robust measure (not affected by feature correlations)\n",
    "#      - Higher perm_mean = removing this feature hurts model more\n",
    "#\n",
    "#   4. SHAP Summary Plot: Visual showing:\n",
    "#      - Feature importance (y-axis)\n",
    "#      - Feature impact on predictions (red = increases sales, blue = decreases)\n",
    "#      - Distribution of feature values\n",
    "#\n",
    "# INTERPRETATION:\n",
    "#   - Features at top of perm_df are most critical for sales\n",
    "#   - SHAP plot shows both importance and direction of effect\n",
    "#   - Use these insights to guide product design\n",
    "#\n",
    "# ============================================================================\n",
    "# Linear model coefficients\n",
    "coef_df = pd.DataFrame({'feature': X.columns, 'coef': ridge.coef_})\n",
    "coef_df = coef_df.reindex(coef_df.coef.abs().sort_values(ascending=False).index)\n",
    "display(coef_df.head(20))\n",
    "\n",
    "# Tree-based importance\n",
    "rf_imp = pd.DataFrame({'feature': X.columns, 'rf_importance': rf_reg.feature_importances_}).sort_values('rf_importance', ascending=False)\n",
    "display(rf_imp.head(20))\n",
    "\n",
    "# Permutation importance on test set (expensive but robust)\n",
    "perm = permutation_importance(rf_reg, X_test, y_reg_test, n_repeats=20, random_state=42)\n",
    "perm_df = pd.DataFrame({'feature': X.columns, 'perm_mean': perm.importances_mean}).sort_values('perm_mean', ascending=False)\n",
    "display(perm_df.head(20))\n",
    "\n",
    "# SHAP (for RandomForest / XGBoost). Use TreeExplainer for tree models, KernelExplainer for Ridge if desired.\n",
    "print(\"Computing SHAP values for XGBoost (may take a moment)...\")\n",
    "explainer = shap.Explainer(xgb_reg)\n",
    "shap_values = explainer(X_test)  # note: shap.Explainer is adaptive\n",
    "# summary plot (global importance)\n",
    "shap.summary_plot(shap_values, X_test, show=True)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 15: CLASSIFICATION MODELS (HIGH vs LOW SALES)\n",
    "# ============================================================================\n",
    "#\n",
    "# PURPOSE:\n",
    "#   - Predict whether product will have high or low sales (binary classification)\n",
    "#   - Compare different classification algorithms\n",
    "#\n",
    "# WHAT WE'RE DOING:\n",
    "#   Training 3 classification models:\n",
    "#   1. Random Forest Classifier: Tree-based ensemble\n",
    "#   2. XGBoost Classifier: Gradient boosting\n",
    "#   3. Logistic Regression: Linear classifier baseline\n",
    "#\n",
    "# EVALUATION METRICS:\n",
    "#   - Accuracy: % of correct predictions\n",
    "#   - Classification Report: Precision, Recall, F1-score for each class\n",
    "#   - Confusion Matrix: Visual showing true vs predicted labels\n",
    "#\n",
    "# EXPECTED RESULT:\n",
    "#   For each model:\n",
    "#   --- RF Classifier ---\n",
    "#   Accuracy: [0-1]\n",
    "#   Classification Report:\n",
    "#                 precision    recall  f1-score   support\n",
    "#           0        X.XX       X.XX      X.XX         X\n",
    "#           1        X.XX       X.XX      X.XX         X\n",
    "#   \n",
    "#   Confusion Matrix Heatmap:\n",
    "#   - Shows: True Negatives, False Positives, False Negatives, True Positives\n",
    "#\n",
    "# INTERPRETATION:\n",
    "#   - Accuracy > 0.5 = better than random guessing\n",
    "#   - Precision = Of predicted high-sales, how many actually high?\n",
    "#   - Recall = Of actual high-sales, how many did we catch?\n",
    "#   - Confusion matrix shows where model makes mistakes\n",
    "#\n",
    "# ============================================================================\n",
    "# We'll train RandomForestClassifier and XGBoost classifier + logistic baseline\n",
    "rf_clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators=200, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "logreg = LogisticRegression(max_iter=500)\n",
    "\n",
    "rf_clf.fit(X_train, y_clf_train)\n",
    "xgb_clf.fit(X_train, y_clf_train)\n",
    "logreg.fit(X_train, y_clf_train)\n",
    "\n",
    "def eval_clf(model, X_tr, X_te, y_tr, y_te, name='clf'):\n",
    "    ypred = model.predict(X_te)\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_te, ypred))\n",
    "    print(classification_report(y_te, ypred))\n",
    "    cm = confusion_matrix(y_te, ypred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d'); plt.title(f\"{name} confusion\"); plt.show()\n",
    "\n",
    "eval_clf(rf_clf, X_train, X_test, y_clf_train, y_clf_test, \"RF Classifier\")\n",
    "eval_clf(xgb_clf, X_train, X_test, y_clf_train, y_clf_test, \"XGBoost Classifier\")\n",
    "eval_clf(logreg, X_train, X_test, y_clf_train, y_clf_test, \"Logistic Regression\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 16: FEATURE IMPORTANCE ANALYSIS (CLASSIFICATION)\n",
    "# ============================================================================\n",
    "#\n",
    "# PURPOSE:\n",
    "#   - Identify features that distinguish high-sales from low-sales products\n",
    "#   - Understand visual differences between successful and unsuccessful products\n",
    "#\n",
    "# WHAT WE'RE DOING:\n",
    "#   1. Random Forest Importance: Which features trees use most\n",
    "#   2. Permutation Importance: Robust measure for classification\n",
    "#   3. SHAP Values: Explain classification decisions\n",
    "#\n",
    "# EXPECTED RESULT:\n",
    "#   1. rf_clf_imp: Top 20 features for classification\n",
    "#   2. perm_clf_df: Top 20 features by permutation importance\n",
    "#   3. SHAP Summary Plot: Shows which features push predictions toward high vs low sales\n",
    "#\n",
    "# INTERPRETATION:\n",
    "#   - Features at top help distinguish high-sales from low-sales\n",
    "#   - SHAP shows: red (high feature value) pushes toward high sales or low sales\n",
    "#   - Use to understand what visual elements make products successful\n",
    "#\n",
    "# ============================================================================\n",
    "rf_clf_imp = pd.DataFrame({'feature': X.columns, 'importance': rf_clf.feature_importances_}).sort_values('importance', ascending=False)\n",
    "display(rf_clf_imp.head(20))\n",
    "\n",
    "perm_clf = permutation_importance(rf_clf, X_test, y_clf_test, n_repeats=20, random_state=42)\n",
    "perm_clf_df = pd.DataFrame({'feature': X.columns, 'perm_mean': perm_clf.importances_mean}).sort_values('perm_mean', ascending=False)\n",
    "display(perm_clf_df.head(20))\n",
    "\n",
    "# SHAP for classifier\n",
    "explainer_clf = shap.Explainer(rf_clf)\n",
    "shap_values_clf = explainer_clf(X_test)\n",
    "shap.summary_plot(shap_values_clf, X_test)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 17: VISUALIZATION - TOP FEATURES VS SALES\n",
    "# ============================================================================\n",
    "#\n",
    "# PURPOSE:\n",
    "#   - Create scatter plots showing relationship between top features and sales\n",
    "#   - Visualize how features relate to sales outcomes\n",
    "#\n",
    "# WHAT WE'RE DOING:\n",
    "#   - For each of top 8 features (from permutation importance):\n",
    "#     1. Create scatter plot: Feature value (x-axis) vs Sales (y-axis)\n",
    "#     2. Each point = one product\n",
    "#     3. Show trend/relationship\n",
    "#\n",
    "# EXPECTED RESULT:\n",
    "#   - 8 scatter plots displayed\n",
    "#   - Each plot shows:\n",
    "#     * X-axis: Feature value\n",
    "#     * Y-axis: Sales (continuous)\n",
    "#     * Points: Individual products\n",
    "#\n",
    "# INTERPRETATION:\n",
    "#   - Upward trend = higher feature → higher sales (positive correlation)\n",
    "#   - Downward trend = higher feature → lower sales (negative correlation)\n",
    "#   - No clear trend = feature not strongly related to sales\n",
    "#   - Outliers = products that don't follow the pattern\n",
    "#\n",
    "# USE CASES:\n",
    "#   - Identify optimal feature values for maximum sales\n",
    "#   - Find products that are outliers (unexpected sales given features)\n",
    "#   - Validate feature importance findings\n",
    "#\n",
    "# ============================================================================\n",
    "top_feats = perm_df.head(8)['feature'].tolist()\n",
    "for f in top_feats:\n",
    "    plt.figure(figsize=(5,3))\n",
    "    sns.scatterplot(x=full[f], y=full['Sale'])\n",
    "    plt.title(f\"Sale vs {f}\")\n",
    "    plt.xlabel(f)\n",
    "    plt.ylabel(\"Sale\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 18: MODEL & DATA SAVING\n",
    "# ============================================================================\n",
    "#\n",
    "# PURPOSE:\n",
    "#   - Save trained models for future use (prediction on new products)\n",
    "#   - Save feature table for analysis or sharing\n",
    "#\n",
    "# WHAT WE'RE DOING:\n",
    "#   1. Save trained models as .py files:\n",
    "#      - rf_reg.py: Random Forest regression model\n",
    "#      - xgb_reg.py: XGBoost regression model\n",
    "#      - rf_clf.py: Random Forest classifier\n",
    "#      - scaler.py: StandardScaler (needed to preprocess new data)\n",
    "#   2. Save full feature table as CSV\n",
    "#\n",
    "# EXPECTED RESULT:\n",
    "#   - Files saved in OUTPUT_DIR\n",
    "#   - Print: \"Saved models as .py files in [path] and feature table [path]\"\n",
    "#\n",
    "# FUTURE USE:\n",
    "#   - Import models from .py files to predict sales for new product images\n",
    "#   - Use feature table for further analysis\n",
    "#\n",
    "# ============================================================================\n",
    "import joblib\n",
    "import base64\n",
    "import pickle\n",
    "\n",
    "def save_model_as_py(model, model_name, output_dir):\n",
    "    \"\"\"Save a trained model as a Python file with embedded model state.\"\"\"\n",
    "    # Serialize model to bytes\n",
    "    model_bytes = pickle.dumps(model)\n",
    "    model_b64 = base64.b64encode(model_bytes).decode('utf-8')\n",
    "    \n",
    "    # Get model class and parameters for documentation\n",
    "    model_class = model.__class__.__name__\n",
    "    model_module = model.__class__.__module__\n",
    "    model_params = model.get_params()\n",
    "    \n",
    "    # Create Python file content\n",
    "    py_content = f'''\"\"\"\n",
    "{model_name} - {model_class} Model\n",
    "Generated from ProteinData.ipynb\n",
    "\n",
    "This file contains a trained {model_class} model that can be loaded and used for predictions.\n",
    "\n",
    "Usage:\n",
    "    from {model_name} import load_model\n",
    "    model = load_model()\n",
    "    predictions = model.predict(X)\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import base64\n",
    "from {model_module} import {model_class}\n",
    "\n",
    "# Model parameters used during training\n",
    "MODEL_PARAMS = {model_params}\n",
    "\n",
    "# Base64 encoded model state\n",
    "MODEL_STATE_B64 = \"\"\"{model_b64}\"\"\"\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Load the trained model from the embedded state.\n",
    "    \n",
    "    Returns:\n",
    "        Trained {model_class} model ready for predictions\n",
    "    \"\"\"\n",
    "    model_bytes = base64.b64decode(MODEL_STATE_B64.encode('utf-8'))\n",
    "    model = pickle.loads(model_bytes)\n",
    "    return model\n",
    "\n",
    "def get_model_info():\n",
    "    \"\"\"\n",
    "    Get information about the model.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Model class name, module, and parameters\n",
    "    \"\"\"\n",
    "    return {{\n",
    "        'model_class': '{model_class}',\n",
    "        'model_module': '{model_module}',\n",
    "        'parameters': MODEL_PARAMS\n",
    "    }}\n",
    "\n",
    "# For backward compatibility, create model instance on import\n",
    "_model_instance = None\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"Get or create model instance (lazy loading).\"\"\"\n",
    "    global _model_instance\n",
    "    if _model_instance is None:\n",
    "        _model_instance = load_model()\n",
    "    return _model_instance\n",
    "'''\n",
    "    \n",
    "    # Write to file\n",
    "    output_path = output_dir / f'{model_name}.py'\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(py_content)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Save all models as .py files\n",
    "print(\"Saving models as .py files...\")\n",
    "rf_reg_path = save_model_as_py(rf_reg, 'rf_reg', OUTPUT_DIR)\n",
    "xgb_reg_path = save_model_as_py(xgb_reg, 'xgb_reg', OUTPUT_DIR)\n",
    "rf_clf_path = save_model_as_py(rf_clf, 'rf_clf', OUTPUT_DIR)\n",
    "scaler_path = save_model_as_py(scaler, 'scaler', OUTPUT_DIR)\n",
    "\n",
    "# Save feature table\n",
    "full.to_csv(OUTPUT_DIR /'feature_table_with_metadata.csv', index=False)\n",
    "\n",
    "print(f\"Saved models as .py files:\")\n",
    "print(f\"  - {rf_reg_path}\")\n",
    "print(f\"  - {xgb_reg_path}\")\n",
    "print(f\"  - {rf_clf_path}\")\n",
    "print(f\"  - {scaler_path}\")\n",
    "print(f\"Saved feature table: {OUTPUT_DIR /'feature_table_with_metadata.csv'}\")\n",
    "\n",
    "# ---------- 19. Quick guided tasks for you (one-by-one) ----------\n",
    "# 1) Run the notebook cells. After the models train, look at: `perm_df` (regression permutation importance) and `perm_clf_df` (classification).\n",
    "# 2) Tell me the top 3 features from permutation importance for regression. We'll then dig into those features specifically (visual examples, grouped statistics).\n",
    "# 3) If any feature surprises you, say which and I will create targeted plots and generate human-readable explanations and potential experiments (e.g., remove text, change color, modify layout).\n",
    "#\n",
    "# End of notebook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein_env (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
