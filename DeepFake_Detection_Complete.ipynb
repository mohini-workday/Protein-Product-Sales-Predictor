{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Fake Detection Project\n",
        "## Complete Pipeline: Data Analysis → Feature Engineering → Model Training → Hyperparameter Tuning\n",
        "\n",
        "**Dataset**: [Hemgg/deep-fake-detection-dfd-entire-original-dataset](https://huggingface.co/datasets/Hemgg/deep-fake-detection-dfd-entire-original-dataset)\n",
        "\n",
        "**Objective**: Detect original vs AI-generated images and videos\n",
        "\n",
        "**Approach**:\n",
        "- Comprehensive EDA\n",
        "- Feature engineering (spatial, frequency, texture features)\n",
        "- Multiple CNN architectures + Transfer Learning\n",
        "- Hyperparameter optimization\n",
        "- Model evaluation and comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install -q datasets huggingface_hub\n",
        "%pip install -q opencv-python-headless\n",
        "%pip install -q scikit-learn\n",
        "%pip install -q matplotlib seaborn\n",
        "%pip install -q pillow\n",
        "%pip install -q torch torchvision torchaudio\n",
        "%pip install -q timm  # PyTorch Image Models\n",
        "%pip install -q optuna  # Hyperparameter tuning\n",
        "%pip install -q scikit-image  # Image processing\n",
        "%pip install -q av  # PyAV for video decoding (alternative to torchcodec, easier to install)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Using CPU - training will be slower\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep Learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "import timm\n",
        "\n",
        "# Computer Vision\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from skimage import feature, filters\n",
        "from skimage.feature import local_binary_pattern\n",
        "\n",
        "# ML & Evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, \n",
        "    f1_score, confusion_matrix, classification_report,\n",
        "    roc_curve, auc, roc_auc_score\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# HuggingFace\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Hyperparameter Tuning\n",
        "import optuna\n",
        "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
        "\n",
        "# Utilities\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "from datetime import datetime\n",
        "import json\n",
        "import joblib\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"Using CPU - training will be slower\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Initial Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "LOADING DATASET FROM HUGGINGFACE (200 RECORDS ONLY)\n",
            "================================================================================\n",
            "\n",
            "[INFO] Loading dataset in streaming mode (only 200 records)...\n",
            "[INFO] This prevents downloading all 3431 files from the dataset\n",
            "[INFO] Extracting first 200 records...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading samples:   0%|          | 0/200 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✗ Error with streaming mode: Could not load libtorchcodec. Likely causes:\n",
            "          1. FFmpeg is not properly installed in your environment. We support\n",
            "             versions 4, 5, 6, and 7 on all platforms, and 8 on Mac and Linux.\n",
            "          2. The PyTorch version (2.9.1) is not compatible with\n",
            "             this version of TorchCodec. Refer to the version compatibility\n",
            "             table:\n",
            "             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\n",
            "          3. Another runtime dependency; see exceptions below.\n",
            "        The following exceptions were raised as we tried to load libtorchcodec:\n",
            "        \n",
            "[start of libtorchcodec loading traceback]\n",
            "FFmpeg version 8: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core8.dylib\n",
            "FFmpeg version 7: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core7.dylib\n",
            "FFmpeg version 6: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core6.dylib\n",
            "FFmpeg version 5: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core5.dylib\n",
            "FFmpeg version 4: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core4.dylib\n",
            "[end of libtorchcodec loading traceback].\n",
            "\n",
            "[INFO] Trying alternative method...\n",
            "[INFO] Attempting alternative: using take() method...\n",
            "\n",
            "✗ Both methods failed: Could not load libtorchcodec. Likely causes:\n",
            "          1. FFmpeg is not properly installed in your environment. We support\n",
            "             versions 4, 5, 6, and 7 on all platforms, and 8 on Mac and Linux.\n",
            "          2. The PyTorch version (2.9.1) is not compatible with\n",
            "             this version of TorchCodec. Refer to the version compatibility\n",
            "             table:\n",
            "             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\n",
            "          3. Another runtime dependency; see exceptions below.\n",
            "        The following exceptions were raised as we tried to load libtorchcodec:\n",
            "        \n",
            "[start of libtorchcodec loading traceback]\n",
            "FFmpeg version 8: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core8.dylib\n",
            "FFmpeg version 7: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core7.dylib\n",
            "FFmpeg version 6: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core6.dylib\n",
            "FFmpeg version 5: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core5.dylib\n",
            "FFmpeg version 4: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core4.dylib\n",
            "[end of libtorchcodec loading traceback].\n",
            "\n",
            "Troubleshooting:\n",
            "1. Check internet connection\n",
            "2. Try: huggingface-cli login\n",
            "3. The dataset might not support streaming mode\n",
            "4. You may need to download the full dataset first, then use .select(range(200))\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/var/folders/c7/vc07w9ls269gkhfrv1_fv0r80000gp/T/ipykernel_77115/4235997940.py\", line 24, in <module>\n",
            "    for i, sample in enumerate(tqdm(dataset_stream, desc=\"Loading samples\", total=MAX_RECORDS)):\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/tqdm/std.py\", line 1181, in __iter__\n",
            "    for obj in iterable:\n",
            "               ^^^^^^^^\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/datasets/iterable_dataset.py\", line 2538, in __iter__\n",
            "    for key, example in ex_iterable:\n",
            "                        ^^^^^^^^^^^\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/datasets/iterable_dataset.py\", line 2056, in __iter__\n",
            "    batch = formatter.format_batch(pa_table)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/datasets/formatting/formatting.py\", line 472, in format_batch\n",
            "    batch = self.python_features_decoder.decode_batch(batch)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/datasets/formatting/formatting.py\", line 234, in decode_batch\n",
            "    return self.features.decode_batch(batch, token_per_repo_id=self.token_per_repo_id) if self.features else batch\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/datasets/features/features.py\", line 2156, in decode_batch\n",
            "    decode_nested_example(self[column_name], value, token_per_repo_id=token_per_repo_id)\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/datasets/features/features.py\", line 1414, in decode_nested_example\n",
            "    return schema.decode_example(obj, token_per_repo_id=token_per_repo_id) if obj is not None else None\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/datasets/features/video.py\", line 179, in decode_example\n",
            "    from torchcodec.decoders import VideoDecoder\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/__init__.py\", line 10, in <module>\n",
            "    from . import decoders, samplers  # noqa\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/decoders/__init__.py\", line 7, in <module>\n",
            "    from .._core import AudioStreamMetadata, VideoStreamMetadata\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/_core/__init__.py\", line 8, in <module>\n",
            "    from ._metadata import (\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/_core/_metadata.py\", line 16, in <module>\n",
            "    from torchcodec._core.ops import (\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/_core/ops.py\", line 84, in <module>\n",
            "    load_torchcodec_shared_libraries()\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/_core/ops.py\", line 69, in load_torchcodec_shared_libraries\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Could not load libtorchcodec. Likely causes:\n",
            "          1. FFmpeg is not properly installed in your environment. We support\n",
            "             versions 4, 5, 6, and 7 on all platforms, and 8 on Mac and Linux.\n",
            "          2. The PyTorch version (2.9.1) is not compatible with\n",
            "             this version of TorchCodec. Refer to the version compatibility\n",
            "             table:\n",
            "             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\n",
            "          3. Another runtime dependency; see exceptions below.\n",
            "        The following exceptions were raised as we tried to load libtorchcodec:\n",
            "        \n",
            "[start of libtorchcodec loading traceback]\n",
            "FFmpeg version 8: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core8.dylib\n",
            "FFmpeg version 7: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core7.dylib\n",
            "FFmpeg version 6: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core6.dylib\n",
            "FFmpeg version 5: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core5.dylib\n",
            "FFmpeg version 4: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core4.dylib\n",
            "[end of libtorchcodec loading traceback].\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/var/folders/c7/vc07w9ls269gkhfrv1_fv0r80000gp/T/ipykernel_77115/4235997940.py\", line 66, in <module>\n",
            "    train_data_list = list(train_data)\n",
            "                      ^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/datasets/iterable_dataset.py\", line 2538, in __iter__\n",
            "    for key, example in ex_iterable:\n",
            "                        ^^^^^^^^^^^\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/datasets/iterable_dataset.py\", line 2056, in __iter__\n",
            "    batch = formatter.format_batch(pa_table)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/datasets/formatting/formatting.py\", line 472, in format_batch\n",
            "    batch = self.python_features_decoder.decode_batch(batch)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/datasets/formatting/formatting.py\", line 234, in decode_batch\n",
            "    return self.features.decode_batch(batch, token_per_repo_id=self.token_per_repo_id) if self.features else batch\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/datasets/features/features.py\", line 2156, in decode_batch\n",
            "    decode_nested_example(self[column_name], value, token_per_repo_id=token_per_repo_id)\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/datasets/features/features.py\", line 1414, in decode_nested_example\n",
            "    return schema.decode_example(obj, token_per_repo_id=token_per_repo_id) if obj is not None else None\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/datasets/features/video.py\", line 179, in decode_example\n",
            "    from torchcodec.decoders import VideoDecoder\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/__init__.py\", line 10, in <module>\n",
            "    from . import decoders, samplers  # noqa\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/decoders/__init__.py\", line 7, in <module>\n",
            "    from .._core import AudioStreamMetadata, VideoStreamMetadata\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/_core/__init__.py\", line 8, in <module>\n",
            "    from ._metadata import (\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/_core/_metadata.py\", line 16, in <module>\n",
            "    from torchcodec._core.ops import (\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/_core/ops.py\", line 84, in <module>\n",
            "    load_torchcodec_shared_libraries()\n",
            "  File \"/Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/_core/ops.py\", line 69, in load_torchcodec_shared_libraries\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Could not load libtorchcodec. Likely causes:\n",
            "          1. FFmpeg is not properly installed in your environment. We support\n",
            "             versions 4, 5, 6, and 7 on all platforms, and 8 on Mac and Linux.\n",
            "          2. The PyTorch version (2.9.1) is not compatible with\n",
            "             this version of TorchCodec. Refer to the version compatibility\n",
            "             table:\n",
            "             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\n",
            "          3. Another runtime dependency; see exceptions below.\n",
            "        The following exceptions were raised as we tried to load libtorchcodec:\n",
            "        \n",
            "[start of libtorchcodec loading traceback]\n",
            "FFmpeg version 8: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core8.dylib\n",
            "FFmpeg version 7: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core7.dylib\n",
            "FFmpeg version 6: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core6.dylib\n",
            "FFmpeg version 5: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core5.dylib\n",
            "FFmpeg version 4: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core4.dylib\n",
            "[end of libtorchcodec loading traceback].\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Could not load libtorchcodec. Likely causes:\n          1. FFmpeg is not properly installed in your environment. We support\n             versions 4, 5, 6, and 7 on all platforms, and 8 on Mac and Linux.\n          2. The PyTorch version (2.9.1) is not compatible with\n             this version of TorchCodec. Refer to the version compatibility\n             table:\n             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\n          3. Another runtime dependency; see exceptions below.\n        The following exceptions were raised as we tried to load libtorchcodec:\n        \n[start of libtorchcodec loading traceback]\nFFmpeg version 8: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core8.dylib\nFFmpeg version 7: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core7.dylib\nFFmpeg version 6: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core6.dylib\nFFmpeg version 5: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core5.dylib\nFFmpeg version 4: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core4.dylib\n[end of libtorchcodec loading traceback].",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 24\u001b[0m\n\u001b[1;32m     23\u001b[0m train_data_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 24\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLoading samples\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_RECORDS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mMAX_RECORDS\u001b[49m\u001b[43m:\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/datasets/iterable_dataset.py:2538\u001b[0m, in \u001b[0;36mIterableDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 2538\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# no need to format thanks to FormattedExamplesIterable\u001b[39;49;00m\n\u001b[1;32m   2540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/datasets/iterable_dataset.py:2056\u001b[0m, in \u001b[0;36mFormattedExamplesIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, pa_table \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_arrow():\n\u001b[0;32m-> 2056\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2057\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m _batch_to_examples(batch):\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/datasets/formatting/formatting.py:472\u001b[0m, in \u001b[0;36mPythonFormatter.format_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    471\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_batch(pa_table)\n\u001b[0;32m--> 472\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_features_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/datasets/formatting/formatting.py:234\u001b[0m, in \u001b[0;36mPythonFeaturesDecoder.decode_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01melse\u001b[39;00m batch\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/datasets/features/features.py:2156\u001b[0m, in \u001b[0;36mFeatures.decode_batch\u001b[0;34m(self, batch, token_per_repo_id)\u001b[0m\n\u001b[1;32m   2153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column_name, column \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2154\u001b[0m     decoded_batch[column_name] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2155\u001b[0m         [\n\u001b[0;32m-> 2156\u001b[0m             \u001b[43mdecode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2157\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2158\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2159\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m column\n\u001b[1;32m   2160\u001b[0m         ]\n\u001b[1;32m   2161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[1;32m   2162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m column\n\u001b[1;32m   2163\u001b[0m     )\n\u001b[1;32m   2164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoded_batch\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/datasets/features/features.py:1414\u001b[0m, in \u001b[0;36mdecode_nested_example\u001b[0;34m(schema, obj, token_per_repo_id)\u001b[0m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(schema, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecode_example\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(schema, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;66;03m# we pass the token to read and decode files from private repositories in streaming mode\u001b[39;00m\n\u001b[0;32m-> 1414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/datasets/features/video.py:179\u001b[0m, in \u001b[0;36mVideo.decode_example\u001b[0;34m(self, value, token_per_repo_id)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mTORCHCODEC_AVAILABLE:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchcodec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecoders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VideoDecoder\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torchcodec/__init__.py:10\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_frame\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioSamples, Frame, FrameBatch  \u001b[38;5;66;03m# usort:skip # noqa\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m decoders, samplers  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Note that version.py is generated during install.\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torchcodec/decoders/__init__.py:7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioStreamMetadata, VideoStreamMetadata\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_audio_decoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioDecoder  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torchcodec/_core/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     AudioStreamMetadata,\n\u001b[1;32m     10\u001b[0m     ContainerMetadata,\n\u001b[1;32m     11\u001b[0m     get_container_metadata,\n\u001b[1;32m     12\u001b[0m     get_container_metadata_from_header,\n\u001b[1;32m     13\u001b[0m     VideoStreamMetadata,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     _add_video_stream,\n\u001b[1;32m     17\u001b[0m     _get_backend_details,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     seek_to_pts,\n\u001b[1;32m     44\u001b[0m )\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torchcodec/_core/_metadata.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchcodec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     _get_container_json_metadata,\n\u001b[1;32m     18\u001b[0m     _get_stream_json_metadata,\n\u001b[1;32m     19\u001b[0m     create_from_file,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     23\u001b[0m SPACES \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torchcodec/_core/ops.py:84\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mCould not load libtorchcodec. Likely causes:\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124m          1. FFmpeg is not properly installed in your environment. We support\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m     )\n\u001b[0;32m---> 84\u001b[0m \u001b[43mload_torchcodec_shared_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Note: We use disallow_in_graph because PyTorch does constant propagation of\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# factory functions.\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torchcodec/_core/ops.py:69\u001b[0m, in \u001b[0;36mload_torchcodec_shared_libraries\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m traceback \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[start of libtorchcodec loading traceback]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFFmpeg version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v, e \u001b[38;5;129;01min\u001b[39;00m exceptions)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[end of libtorchcodec loading traceback].\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m )\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mCould not load libtorchcodec. Likely causes:\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124m      1. FFmpeg is not properly installed in your environment. We support\u001b[39m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124m         versions 4, 5, 6, and 7 on all platforms, and 8 on Mac and Linux.\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124m      2. The PyTorch version (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is not compatible with\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124m         this version of TorchCodec. Refer to the version compatibility\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124m         table:\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124m         https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\u001b[39m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124m      3. Another runtime dependency; see exceptions below.\u001b[39m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124m    The following exceptions were raised as we tried to load libtorchcodec:\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Could not load libtorchcodec. Likely causes:\n          1. FFmpeg is not properly installed in your environment. We support\n             versions 4, 5, 6, and 7 on all platforms, and 8 on Mac and Linux.\n          2. The PyTorch version (2.9.1) is not compatible with\n             this version of TorchCodec. Refer to the version compatibility\n             table:\n             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\n          3. Another runtime dependency; see exceptions below.\n        The following exceptions were raised as we tried to load libtorchcodec:\n        \n[start of libtorchcodec loading traceback]\nFFmpeg version 8: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core8.dylib\nFFmpeg version 7: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core7.dylib\nFFmpeg version 6: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core6.dylib\nFFmpeg version 5: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core5.dylib\nFFmpeg version 4: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core4.dylib\n[end of libtorchcodec loading traceback].",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m train_data \u001b[38;5;241m=\u001b[39m dataset_stream\u001b[38;5;241m.\u001b[39mtake(MAX_RECORDS)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Convert to list and back to Dataset\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m train_data_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     68\u001b[0m train_data \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_list(train_data_list)\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/datasets/iterable_dataset.py:2538\u001b[0m, in \u001b[0;36mIterableDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2535\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m formatter\u001b[38;5;241m.\u001b[39mformat_row(pa_table)\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 2538\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# no need to format thanks to FormattedExamplesIterable\u001b[39;49;00m\n\u001b[1;32m   2540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/datasets/iterable_dataset.py:2056\u001b[0m, in \u001b[0;36mFormattedExamplesIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mex_iterable\u001b[38;5;241m.\u001b[39miter_arrow:\n\u001b[1;32m   2054\u001b[0m     \u001b[38;5;66;03m# feature casting (inc column addition) handled within self._iter_arrow()\u001b[39;00m\n\u001b[1;32m   2055\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, pa_table \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_arrow():\n\u001b[0;32m-> 2056\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2057\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m _batch_to_examples(batch):\n\u001b[1;32m   2058\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m key, example\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/datasets/formatting/formatting.py:472\u001b[0m, in \u001b[0;36mPythonFormatter.format_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyBatch(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    471\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_batch(pa_table)\n\u001b[0;32m--> 472\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_features_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/datasets/formatting/formatting.py:234\u001b[0m, in \u001b[0;36mPythonFeaturesDecoder.decode_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01melse\u001b[39;00m batch\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/datasets/features/features.py:2156\u001b[0m, in \u001b[0;36mFeatures.decode_batch\u001b[0;34m(self, batch, token_per_repo_id)\u001b[0m\n\u001b[1;32m   2152\u001b[0m decoded_batch \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column_name, column \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2154\u001b[0m     decoded_batch[column_name] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2155\u001b[0m         [\n\u001b[0;32m-> 2156\u001b[0m             \u001b[43mdecode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2157\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2158\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2159\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m column\n\u001b[1;32m   2160\u001b[0m         ]\n\u001b[1;32m   2161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[1;32m   2162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m column\n\u001b[1;32m   2163\u001b[0m     )\n\u001b[1;32m   2164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoded_batch\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/datasets/features/features.py:1414\u001b[0m, in \u001b[0;36mdecode_nested_example\u001b[0;34m(schema, obj, token_per_repo_id)\u001b[0m\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;66;03m# Object with special decoding:\u001b[39;00m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(schema, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecode_example\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(schema, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;66;03m# we pass the token to read and decode files from private repositories in streaming mode\u001b[39;00m\n\u001b[0;32m-> 1414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/datasets/features/video.py:179\u001b[0m, in \u001b[0;36mVideo.decode_example\u001b[0;34m(self, value, token_per_repo_id)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecoding is disabled for this feature. Please use Video(decode=True) instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mTORCHCODEC_AVAILABLE:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchcodec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecoders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VideoDecoder\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo support decoding videos, please install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorchcodec\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torchcodec/__init__.py:10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Note: usort wants to put Frame and FrameBatch after decoders and samplers,\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# but that results in circular import.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_frame\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioSamples, Frame, FrameBatch  \u001b[38;5;66;03m# usort:skip # noqa\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m decoders, samplers  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Note that version.py is generated during install.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torchcodec/decoders/__init__.py:7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioStreamMetadata, VideoStreamMetadata\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_audio_decoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioDecoder  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decoder_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m set_cuda_backend  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torchcodec/_core/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     AudioStreamMetadata,\n\u001b[1;32m     10\u001b[0m     ContainerMetadata,\n\u001b[1;32m     11\u001b[0m     get_container_metadata,\n\u001b[1;32m     12\u001b[0m     get_container_metadata_from_header,\n\u001b[1;32m     13\u001b[0m     VideoStreamMetadata,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     _add_video_stream,\n\u001b[1;32m     17\u001b[0m     _get_backend_details,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     seek_to_pts,\n\u001b[1;32m     44\u001b[0m )\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torchcodec/_core/_metadata.py:16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Union\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchcodec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     _get_container_json_metadata,\n\u001b[1;32m     18\u001b[0m     _get_stream_json_metadata,\n\u001b[1;32m     19\u001b[0m     create_from_file,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     23\u001b[0m SPACES \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mStreamMetadata\u001b[39;00m:\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torchcodec/_core/ops.py:84\u001b[0m\n\u001b[1;32m     64\u001b[0m     traceback \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[start of libtorchcodec loading traceback]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFFmpeg version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v, e \u001b[38;5;129;01min\u001b[39;00m exceptions)\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[end of libtorchcodec loading traceback].\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mCould not load libtorchcodec. Likely causes:\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124m          1. FFmpeg is not properly installed in your environment. We support\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m     )\n\u001b[0;32m---> 84\u001b[0m \u001b[43mload_torchcodec_shared_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Note: We use disallow_in_graph because PyTorch does constant propagation of\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# factory functions.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m create_from_file \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisallow_in_graph(\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mtorchcodec_ns\u001b[38;5;241m.\u001b[39mcreate_from_file\u001b[38;5;241m.\u001b[39mdefault\n\u001b[1;32m     91\u001b[0m )\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torchcodec/_core/ops.py:69\u001b[0m, in \u001b[0;36mload_torchcodec_shared_libraries\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m         exceptions\u001b[38;5;241m.\u001b[39mappend((ffmpeg_major_version, e))\n\u001b[1;32m     64\u001b[0m traceback \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[start of libtorchcodec loading traceback]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFFmpeg version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v, e \u001b[38;5;129;01min\u001b[39;00m exceptions)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[end of libtorchcodec loading traceback].\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m )\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mCould not load libtorchcodec. Likely causes:\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124m      1. FFmpeg is not properly installed in your environment. We support\u001b[39m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124m         versions 4, 5, 6, and 7 on all platforms, and 8 on Mac and Linux.\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124m      2. The PyTorch version (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is not compatible with\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124m         this version of TorchCodec. Refer to the version compatibility\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124m         table:\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124m         https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\u001b[39m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124m      3. Another runtime dependency; see exceptions below.\u001b[39m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124m    The following exceptions were raised as we tried to load libtorchcodec:\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Could not load libtorchcodec. Likely causes:\n          1. FFmpeg is not properly installed in your environment. We support\n             versions 4, 5, 6, and 7 on all platforms, and 8 on Mac and Linux.\n          2. The PyTorch version (2.9.1) is not compatible with\n             this version of TorchCodec. Refer to the version compatibility\n             table:\n             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\n          3. Another runtime dependency; see exceptions below.\n        The following exceptions were raised as we tried to load libtorchcodec:\n        \n[start of libtorchcodec loading traceback]\nFFmpeg version 8: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core8.dylib\nFFmpeg version 7: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core7.dylib\nFFmpeg version 6: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core6.dylib\nFFmpeg version 5: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core5.dylib\nFFmpeg version 4: Could not load this library: /Users/mohini.gangaram/Library/Python/3.12/lib/python/site-packages/torchcodec/libtorchcodec_core4.dylib\n[end of libtorchcodec loading traceback]."
          ]
        }
      ],
      "source": [
        "# Load dataset from HuggingFace (only 200 records)\n",
        "print(\"=\"*80)\n",
        "print(\"LOADING DATASET FROM HUGGINGFACE (200 RECORDS ONLY)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "MAX_RECORDS = 200  # Only download first 200 records\n",
        "\n",
        "# Configure video decoding to use av (PyAV) instead of torchcodec\n",
        "import os\n",
        "os.environ['HF_DATASETS_VIDEO_DECODER'] = 'av'  # Use PyAV instead of torchcodec\n",
        "\n",
        "try:\n",
        "    # Method: Use streaming mode to prevent downloading all files\n",
        "    # This will only download/stream the first 200 records, not all 3431 files\n",
        "    print(\"\\n[INFO] Loading dataset in streaming mode (only 200 records)...\")\n",
        "    print(\"[INFO] This prevents downloading all 3431 files from the dataset\")\n",
        "    print(\"[INFO] Using PyAV (av) for video decoding instead of torchcodec\")\n",
        "    \n",
        "    # Load with streaming and take only first 200 records\n",
        "    # Set video decoder to 'av' to avoid torchcodec dependency\n",
        "    dataset_stream = load_dataset(\n",
        "        \"Hemgg/deep-fake-detection-dfd-entire-original-dataset\",\n",
        "        streaming=True,\n",
        "        split=\"train\"\n",
        "    )\n",
        "    \n",
        "    # Take only first 200 records - this prevents downloading all 3431 files\n",
        "    print(f\"[INFO] Extracting first {MAX_RECORDS} records...\")\n",
        "    train_data_list = []\n",
        "    for i, sample in enumerate(tqdm(dataset_stream, desc=\"Loading samples\", total=MAX_RECORDS)):\n",
        "        if i >= MAX_RECORDS:\n",
        "            break\n",
        "        train_data_list.append(sample)\n",
        "    \n",
        "    # Convert to Dataset object for compatibility with rest of code\n",
        "    from datasets import Dataset\n",
        "    train_data = Dataset.from_list(train_data_list)\n",
        "    \n",
        "    print(f\"\\n✓ Dataset loaded successfully!\")\n",
        "    print(f\"✓ Only downloaded {len(train_data)} records (not all 3431 files)\")\n",
        "    print(f\"\\nNumber of samples: {len(train_data)}\")\n",
        "    print(f\"Column names: {train_data.column_names}\")\n",
        "    print(f\"Features: {train_data.features}\")\n",
        "    \n",
        "    # Inspect first sample\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"FIRST SAMPLE INSPECTION\")\n",
        "    print(\"-\"*80)\n",
        "    sample = train_data[0]\n",
        "    for key, value in sample.items():\n",
        "        if key == 'video':\n",
        "            print(f\"{key}: <Video data - type: {type(value)}>\")\n",
        "            if hasattr(value, 'shape'):\n",
        "                print(f\"  Shape: {value.shape}\")\n",
        "            elif isinstance(value, dict):\n",
        "                print(f\"  Video dict keys: {value.keys()}\")\n",
        "        else:\n",
        "            print(f\"{key}: {value}\")\n",
        "            \n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ Error with streaming mode: {e}\")\n",
        "    print(\"\\n[INFO] Trying alternative method without video decoding...\")\n",
        "    \n",
        "    # Fallback: Load without automatic video decoding\n",
        "    try:\n",
        "        print(\"[INFO] Attempting to load dataset without video decoding...\")\n",
        "        # Try loading with video decoding disabled\n",
        "        dataset_stream = load_dataset(\n",
        "            \"Hemgg/deep-fake-detection-dfd-entire-original-dataset\",\n",
        "            streaming=True,\n",
        "            split=\"train\"\n",
        "        )\n",
        "        \n",
        "        # Load samples without decoding videos\n",
        "        train_data_list = []\n",
        "        for i, sample in enumerate(tqdm(dataset_stream, desc=\"Loading samples\", total=MAX_RECORDS)):\n",
        "            if i >= MAX_RECORDS:\n",
        "                break\n",
        "            # Keep video as path/bytes if decoding fails\n",
        "            train_data_list.append(sample)\n",
        "        \n",
        "        from datasets import Dataset\n",
        "        train_data = Dataset.from_list(train_data_list)\n",
        "        print(f\"\\n✓ Dataset loaded successfully (videos may need manual decoding)!\")\n",
        "        print(f\"✓ Only downloaded {len(train_data)} records\")\n",
        "        \n",
        "    except Exception as e2:\n",
        "        print(f\"\\n✗ Error: {e2}\")\n",
        "        print(\"\\nTroubleshooting:\")\n",
        "        print(\"1. Check internet connection\")\n",
        "        print(\"2. Try: huggingface-cli login\")\n",
        "        print(\"3. Install FFmpeg: brew install ffmpeg (on Mac) or apt-get install ffmpeg (on Linux)\")\n",
        "        print(\"4. Or try: pip install ffmpeg-python\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Exploratory Data Analysis (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_frame_from_video(video_data, frame_idx=0):\n",
        "    \"\"\"Extract a frame from video data\"\"\"\n",
        "    try:\n",
        "        if isinstance(video_data, dict):\n",
        "            if 'path' in video_data:\n",
        "                cap = cv2.VideoCapture(video_data['path'])\n",
        "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "                ret, frame = cap.read()\n",
        "                cap.release()\n",
        "                if ret:\n",
        "                    return cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        elif isinstance(video_data, np.ndarray):\n",
        "            if len(video_data.shape) == 4:  # (frames, height, width, channels)\n",
        "                return video_data[frame_idx]\n",
        "            elif len(video_data.shape) == 3:  # Single frame\n",
        "                return video_data\n",
        "        elif hasattr(video_data, 'shape'):\n",
        "            return np.array(video_data)\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting frame: {e}\")\n",
        "    return None\n",
        "\n",
        "def analyze_dataset_structure(dataset, n_samples=100, label_col=None):\n",
        "    \"\"\"Comprehensive dataset analysis\"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"EXPLORATORY DATA ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Convert to pandas for easier analysis\n",
        "    data_list = []\n",
        "    for i in range(min(n_samples, len(dataset))):\n",
        "        sample = dataset[i]\n",
        "        data_list.append(sample)\n",
        "    \n",
        "    df = pd.DataFrame(data_list)\n",
        "    \n",
        "    print(f\"\\n1. DATASET OVERVIEW\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"Total samples analyzed: {len(df)}\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "    print(f\"\\nData types:\\n{df.dtypes}\")\n",
        "    \n",
        "    # Check for label column\n",
        "    label_cols = [col for col in df.columns if 'label' in col.lower() or 'class' in col.lower()]\n",
        "    \n",
        "    if label_cols:\n",
        "        label_col = label_cols[0]\n",
        "        print(f\"\\n2. LABEL DISTRIBUTION (Column: {label_col})\")\n",
        "        print(\"-\"*80)\n",
        "        \n",
        "        # Get all labels from full dataset\n",
        "        all_labels = []\n",
        "        for i in tqdm(range(len(dataset)), desc=\"Extracting labels\"):\n",
        "            sample = dataset[i]\n",
        "            if label_col in sample:\n",
        "                all_labels.append(sample[label_col])\n",
        "        \n",
        "        label_counts = pd.Series(all_labels).value_counts()\n",
        "        label_percentages = pd.Series(all_labels).value_counts(normalize=True) * 100\n",
        "        \n",
        "        print(f\"\\nLabel counts:\\n{label_counts}\")\n",
        "        print(f\"\\nLabel percentages:\\n{label_percentages}\")\n",
        "        \n",
        "        # Visualize distribution\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "        \n",
        "        label_counts.plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
        "        axes[0].set_title('Label Distribution (Count)', fontsize=14, fontweight='bold')\n",
        "        axes[0].set_xlabel('Label', fontsize=12)\n",
        "        axes[0].set_ylabel('Count', fontsize=12)\n",
        "        axes[0].tick_params(axis='x', rotation=0)\n",
        "        \n",
        "        colors = ['#2ecc71', '#e74c3c']\n",
        "        axes[1].pie(label_counts, labels=label_counts.index, autopct='%1.1f%%',\n",
        "                   colors=colors[:len(label_counts)], startangle=90)\n",
        "        axes[1].set_title('Label Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('label_distribution.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        # Class imbalance check\n",
        "        imbalance_ratio = label_counts.max() / label_counts.min()\n",
        "        print(f\"\\n3. CLASS IMBALANCE ANALYSIS\")\n",
        "        print(\"-\"*80)\n",
        "        print(f\"Imbalance Ratio: {imbalance_ratio:.2f}\")\n",
        "        if imbalance_ratio > 1.5:\n",
        "            print(\"⚠ Warning: Significant class imbalance detected!\")\n",
        "            print(\"  → Will use weighted loss function\")\n",
        "        else:\n",
        "            print(\"✓ Classes are relatively balanced\")\n",
        "        \n",
        "        return label_col, label_counts\n",
        "    \n",
        "    return None, None\n",
        "\n",
        "# Run EDA\n",
        "label_column, label_distribution = analyze_dataset_structure(train_data, n_samples=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sample images/videos\n",
        "def visualize_samples(dataset, n_samples=8, label_col=None):\n",
        "    \"\"\"Visualize sample images from the dataset\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"VISUALIZING SAMPLE DATA\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    for idx in range(min(n_samples, len(dataset))):\n",
        "        sample = dataset[idx]\n",
        "        \n",
        "        # Extract image/frame\n",
        "        image = None\n",
        "        if 'video' in sample:\n",
        "            image = extract_frame_from_video(sample['video'])\n",
        "        elif 'image' in sample:\n",
        "            img_data = sample['image']\n",
        "            if isinstance(img_data, Image.Image):\n",
        "                image = np.array(img_data)\n",
        "            elif isinstance(img_data, np.ndarray):\n",
        "                image = img_data\n",
        "        \n",
        "        if image is not None:\n",
        "            axes[idx].imshow(image)\n",
        "            \n",
        "            # Add label if available\n",
        "            title = f\"Sample {idx+1}\"\n",
        "            if label_col and label_col in sample:\n",
        "                title += f\"\\nLabel: {sample[label_col]}\"\n",
        "            axes[idx].set_title(title, fontsize=10)\n",
        "            axes[idx].axis('off')\n",
        "        else:\n",
        "            axes[idx].text(0.5, 0.5, 'Could not\\nload image', \n",
        "                         ha='center', va='center', fontsize=12)\n",
        "            axes[idx].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('sample_images.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Visualize samples\n",
        "if label_column:\n",
        "    visualize_samples(train_data, n_samples=8, label_col=label_column)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeatureExtractor:\n",
        "    \"\"\"Extract handcrafted features from images for deepfake detection\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def extract_spatial_features(image):\n",
        "        \"\"\"Extract spatial domain features\"\"\"\n",
        "        if len(image.shape) == 3:\n",
        "            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "        else:\n",
        "            gray = image\n",
        "        \n",
        "        features = {}\n",
        "        \n",
        "        # Basic statistics\n",
        "        features['mean'] = np.mean(gray)\n",
        "        features['std'] = np.std(gray)\n",
        "        features['var'] = np.var(gray)\n",
        "        features['min'] = np.min(gray)\n",
        "        features['max'] = np.max(gray)\n",
        "        \n",
        "        # Histogram features\n",
        "        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n",
        "        features['hist_entropy'] = -np.sum(hist * np.log(hist + 1e-10))\n",
        "        features['hist_skewness'] = np.sum(((np.arange(256) - features['mean']) ** 3) * hist.flatten()) / (features['std'] ** 3 + 1e-10)\n",
        "        \n",
        "        return features\n",
        "    \n",
        "    @staticmethod\n",
        "    def extract_frequency_features(image):\n",
        "        \"\"\"Extract frequency domain features (FFT)\"\"\"\n",
        "        if len(image.shape) == 3:\n",
        "            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "        else:\n",
        "            gray = image\n",
        "        \n",
        "        # FFT\n",
        "        fft = np.fft.fft2(gray)\n",
        "        fft_shift = np.fft.fftshift(fft)\n",
        "        magnitude = np.abs(fft_shift)\n",
        "        \n",
        "        features = {}\n",
        "        features['fft_mean'] = np.mean(magnitude)\n",
        "        features['fft_std'] = np.std(magnitude)\n",
        "        features['fft_energy'] = np.sum(magnitude ** 2)\n",
        "        \n",
        "        return features\n",
        "    \n",
        "    @staticmethod\n",
        "    def extract_texture_features(image):\n",
        "        \"\"\"Extract texture features using Local Binary Pattern (LBP)\"\"\"\n",
        "        if len(image.shape) == 3:\n",
        "            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "        else:\n",
        "            gray = image\n",
        "        \n",
        "        # LBP\n",
        "        radius = 3\n",
        "        n_points = 8 * radius\n",
        "        lbp = local_binary_pattern(gray, n_points, radius, method='uniform')\n",
        "        \n",
        "        features = {}\n",
        "        features['lbp_mean'] = np.mean(lbp)\n",
        "        features['lbp_std'] = np.std(lbp)\n",
        "        features['lbp_entropy'] = -np.sum((np.histogram(lbp.ravel(), bins=256)[0] + 1e-10) * \n",
        "                                         np.log(np.histogram(lbp.ravel(), bins=256)[0] + 1e-10))\n",
        "        \n",
        "        # GLCM-like features\n",
        "        from skimage.feature import graycomatrix, graycoprops\n",
        "        try:\n",
        "            glcm = graycomatrix(gray.astype(np.uint8), distances=[1], angles=[0], levels=256, symmetric=True, normed=True)\n",
        "            contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
        "            dissimilarity = graycoprops(glcm, 'dissimilarity')[0, 0]\n",
        "            homogeneity = graycoprops(glcm, 'homogeneity')[0, 0]\n",
        "            energy = graycoprops(glcm, 'energy')[0, 0]\n",
        "            \n",
        "            features['glcm_contrast'] = contrast\n",
        "            features['glcm_dissimilarity'] = dissimilarity\n",
        "            features['glcm_homogeneity'] = homogeneity\n",
        "            features['glcm_energy'] = energy\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        return features\n",
        "    \n",
        "    @staticmethod\n",
        "    def extract_color_features(image):\n",
        "        \"\"\"Extract color-based features\"\"\"\n",
        "        if len(image.shape) == 2:\n",
        "            return {}\n",
        "        \n",
        "        features = {}\n",
        "        \n",
        "        # Convert to different color spaces\n",
        "        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
        "        lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
        "        \n",
        "        # RGB statistics\n",
        "        for i, color in enumerate(['R', 'G', 'B']):\n",
        "            features[f'{color}_mean'] = np.mean(image[:, :, i])\n",
        "            features[f'{color}_std'] = np.std(image[:, :, i])\n",
        "        \n",
        "        # HSV statistics\n",
        "        for i, color in enumerate(['H', 'S', 'V']):\n",
        "            features[f'{color}_mean'] = np.mean(hsv[:, :, i])\n",
        "            features[f'{color}_std'] = np.std(hsv[:, :, i])\n",
        "        \n",
        "        return features\n",
        "    \n",
        "    @staticmethod\n",
        "    def extract_all_features(image):\n",
        "        \"\"\"Extract all features\"\"\"\n",
        "        all_features = {}\n",
        "        \n",
        "        all_features.update(FeatureExtractor.extract_spatial_features(image))\n",
        "        all_features.update(FeatureExtractor.extract_frequency_features(image))\n",
        "        all_features.update(FeatureExtractor.extract_texture_features(image))\n",
        "        all_features.update(FeatureExtractor.extract_color_features(image))\n",
        "        \n",
        "        return all_features\n",
        "\n",
        "print(\"✓ Feature extraction functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Preparation and Custom Dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DeepfakeDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for Deepfake Detection\"\"\"\n",
        "    \n",
        "    def __init__(self, hf_dataset, transform=None, max_samples=None, label_col=None, use_features=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hf_dataset: HuggingFace dataset\n",
        "            transform: torchvision transforms\n",
        "            max_samples: Limit number of samples\n",
        "            label_col: Name of label column\n",
        "            use_features: If True, extract handcrafted features instead of images\n",
        "        \"\"\"\n",
        "        self.dataset = hf_dataset\n",
        "        self.transform = transform\n",
        "        self.label_col = label_col\n",
        "        self.use_features = use_features\n",
        "        \n",
        "        if max_samples:\n",
        "            self.dataset = self.dataset.select(range(min(max_samples, len(self.dataset))))\n",
        "        \n",
        "        # Identify label column if not provided\n",
        "        if not self.label_col:\n",
        "            for col in self.dataset.column_names:\n",
        "                if 'label' in col.lower() or 'class' in col.lower():\n",
        "                    self.label_col = col\n",
        "                    break\n",
        "        \n",
        "        print(f\"[INFO] Dataset initialized with {len(self.dataset)} samples\")\n",
        "        if self.label_col:\n",
        "            print(f\"[INFO] Using '{self.label_col}' as label column\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.dataset[idx]\n",
        "        \n",
        "        if self.use_features:\n",
        "            # Extract handcrafted features\n",
        "            image = None\n",
        "            if 'video' in sample:\n",
        "                image = extract_frame_from_video(sample['video'])\n",
        "            elif 'image' in sample:\n",
        "                img_data = sample['image']\n",
        "                if isinstance(img_data, Image.Image):\n",
        "                    image = np.array(img_data)\n",
        "                elif isinstance(img_data, np.ndarray):\n",
        "                    image = img_data\n",
        "            \n",
        "            if image is not None:\n",
        "                features = FeatureExtractor.extract_all_features(image)\n",
        "                features_array = np.array(list(features.values()), dtype=np.float32)\n",
        "                data = torch.FloatTensor(features_array)\n",
        "            else:\n",
        "                data = torch.zeros(20)  # Fallback\n",
        "        else:\n",
        "            # Extract image/frame\n",
        "            image = None\n",
        "            if 'video' in sample:\n",
        "                image = extract_frame_from_video(sample['video'])\n",
        "            elif 'image' in sample:\n",
        "                img_data = sample['image']\n",
        "                if isinstance(img_data, Image.Image):\n",
        "                    image = img_data\n",
        "                elif isinstance(img_data, np.ndarray):\n",
        "                    image = Image.fromarray(img_data)\n",
        "            \n",
        "            if image is None:\n",
        "                # Create dummy image\n",
        "                image = Image.new('RGB', (224, 224), color='black')\n",
        "            \n",
        "            # Apply transforms\n",
        "            if self.transform:\n",
        "                data = self.transform(image)\n",
        "            else:\n",
        "                data = transforms.ToTensor()(image)\n",
        "        \n",
        "        # Get label\n",
        "        if self.label_col and self.label_col in sample:\n",
        "            label = sample[self.label_col]\n",
        "            # Convert label to integer if needed\n",
        "            if isinstance(label, str):\n",
        "                label = 0 if 'real' in label.lower() or 'original' in label.lower() else 1\n",
        "            elif isinstance(label, (int, float)):\n",
        "                label = int(label)\n",
        "            else:\n",
        "                label = 0\n",
        "        else:\n",
        "            label = 0\n",
        "        \n",
        "        return data, label\n",
        "\n",
        "print(\"✓ Custom Dataset class defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "print(\"✓ Transforms defined\")\n",
        "\n",
        "# Create datasets\n",
        "print(\"\\n[INFO] Creating datasets...\")\n",
        "\n",
        "# Limit samples for faster training (set to None for full dataset)\n",
        "MAX_SAMPLES = 1000  # Adjust based on your needs\n",
        "\n",
        "try:\n",
        "    # Create full dataset\n",
        "    full_dataset = DeepfakeDataset(\n",
        "        train_data, \n",
        "        transform=train_transform,\n",
        "        max_samples=MAX_SAMPLES,\n",
        "        label_col=label_column\n",
        "    )\n",
        "    \n",
        "    # Split into train/val/test (70/15/15)\n",
        "    train_size = int(0.7 * len(full_dataset))\n",
        "    val_size = int(0.15 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size - val_size\n",
        "    \n",
        "    train_dataset, val_dataset, test_dataset = random_split(\n",
        "        full_dataset, \n",
        "        [train_size, val_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(SEED)\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n✓ Datasets created successfully!\")\n",
        "    print(f\"  - Training samples: {len(train_dataset)}\")\n",
        "    print(f\"  - Validation samples: {len(val_dataset)}\")\n",
        "    print(f\"  - Test samples: {len(test_dataset)}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ Error creating datasets: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# Create data loaders\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True,\n",
        "    num_workers=2 if torch.cuda.is_available() else 0,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False,\n",
        "    num_workers=2 if torch.cuda.is_available() else 0,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False,\n",
        "    num_workers=2 if torch.cuda.is_available() else 0,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Data loaders created\")\n",
        "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  - Training batches: {len(train_loader)}\")\n",
        "print(f\"  - Validation batches: {len(val_loader)}\")\n",
        "print(f\"  - Test batches: {len(test_loader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Architectures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model 1: Simple CNN\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        \n",
        "        self.features = nn.Sequential(\n",
        "            # Conv Block 1\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            \n",
        "            # Conv Block 2\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            \n",
        "            # Conv Block 3\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            \n",
        "            # Conv Block 4\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Model 2: ResNet18 (Transfer Learning)\n",
        "def create_resnet18(num_classes=2, pretrained=True):\n",
        "    model = models.resnet18(pretrained=pretrained)\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(num_ftrs, 128),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(128, num_classes)\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Model 3: EfficientNet (Transfer Learning)\n",
        "def create_efficientnet(num_classes=2, model_name='efficientnet_b0', pretrained=True):\n",
        "    model = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes)\n",
        "    return model\n",
        "\n",
        "print(\"✓ Model architectures defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for inputs, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader, desc=\"Validating\", leave=False):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            \n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc, all_preds, all_labels\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, lr=0.001, weight_decay=1e-4, class_weights=None):\n",
        "    \"\"\"Train a model\"\"\"\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # Loss and optimizer\n",
        "    if class_weights is not None:\n",
        "        class_weights = torch.FloatTensor(class_weights).to(device)\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
        "    \n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "    \n",
        "    best_val_acc = 0.0\n",
        "    best_model_state = None\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"TRAINING MODEL: {model.__class__.__name__}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        print(\"-\" * 80)\n",
        "        \n",
        "        # Train\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        \n",
        "        # Validate\n",
        "        val_loss, val_acc, _, _ = validate(model, val_loader, criterion, device)\n",
        "        \n",
        "        # Update learning rate\n",
        "        scheduler.step(val_loss)\n",
        "        \n",
        "        # Save history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        \n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = model.state_dict().copy()\n",
        "        \n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "    \n",
        "    # Load best model\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "        print(f\"\\n✓ Best model loaded (Val Acc: {best_val_acc:.2f}%)\")\n",
        "    \n",
        "    return model, history\n",
        "\n",
        "print(\"✓ Training functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Model Training (Baseline Models)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate class weights if needed\n",
        "def calculate_class_weights(dataset, label_col):\n",
        "    \"\"\"Calculate class weights for imbalanced datasets\"\"\"\n",
        "    labels = []\n",
        "    for i in range(len(dataset)):\n",
        "        sample = dataset[i]\n",
        "        if label_col in sample:\n",
        "            label = sample[label_col]\n",
        "            if isinstance(label, str):\n",
        "                label = 0 if 'real' in label.lower() or 'original' in label.lower() else 1\n",
        "            labels.append(int(label))\n",
        "    \n",
        "    from sklearn.utils.class_weight import compute_class_weight\n",
        "    classes = np.unique(labels)\n",
        "    weights = compute_class_weight('balanced', classes=classes, y=labels)\n",
        "    return weights.tolist()\n",
        "\n",
        "# Calculate class weights\n",
        "if label_column:\n",
        "    class_weights = calculate_class_weights(train_data, label_column)\n",
        "    print(f\"Class weights: {class_weights}\")\n",
        "else:\n",
        "    class_weights = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Model 1: Simple CNN\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING MODEL 1: SIMPLE CNN\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "model1 = SimpleCNN(num_classes=2)\n",
        "model1, history1 = train_model(\n",
        "    model1, \n",
        "    train_loader, \n",
        "    val_loader, \n",
        "    num_epochs=10, \n",
        "    lr=0.001,\n",
        "    class_weights=class_weights\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "axes[0].plot(history1['train_loss'], label='Train Loss')\n",
        "axes[0].plot(history1['val_loss'], label='Val Loss')\n",
        "axes[0].set_title('Model 1: Loss')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "axes[1].plot(history1['train_acc'], label='Train Acc')\n",
        "axes[1].plot(history1['val_acc'], label='Val Acc')\n",
        "axes[1].set_title('Model 1: Accuracy')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy (%)')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model1_training_history.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Model 2: ResNet18\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING MODEL 2: RESNET18 (TRANSFER LEARNING)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "model2 = create_resnet18(num_classes=2, pretrained=True)\n",
        "model2, history2 = train_model(\n",
        "    model2, \n",
        "    train_loader, \n",
        "    val_loader, \n",
        "    num_epochs=10, \n",
        "    lr=0.0001,  # Lower LR for transfer learning\n",
        "    class_weights=class_weights\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "axes[0].plot(history2['train_loss'], label='Train Loss')\n",
        "axes[0].plot(history2['val_loss'], label='Val Loss')\n",
        "axes[0].set_title('Model 2: Loss')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "axes[1].plot(history2['train_acc'], label='Train Acc')\n",
        "axes[1].plot(history2['val_acc'], label='Val Acc')\n",
        "axes[1].set_title('Model 2: Accuracy')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy (%)')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model2_training_history.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, model_name=\"Model\"):\n",
        "    \"\"\"Comprehensive model evaluation\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader, desc=f\"Evaluating {model_name}\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(inputs)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            \n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs[:, 1].cpu().numpy())  # Probability of class 1 (fake)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    roc_auc = roc_auc_score(all_labels, all_probs)\n",
        "    \n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"EVALUATION RESULTS: {model_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1-Score:  {f1:.4f}\")\n",
        "    print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
        "    \n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(cm)\n",
        "    \n",
        "    print(f\"\\nClassification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=['Real', 'Fake']))\n",
        "    \n",
        "    # Visualizations\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Confusion Matrix\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0], \n",
        "                xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "    axes[0].set_title(f'{model_name} - Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_ylabel('True Label')\n",
        "    axes[0].set_xlabel('Predicted Label')\n",
        "    \n",
        "    # ROC Curve\n",
        "    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
        "    axes[1].plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})', linewidth=2)\n",
        "    axes[1].plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "    axes[1].set_xlabel('False Positive Rate')\n",
        "    axes[1].set_ylabel('True Positive Rate')\n",
        "    axes[1].set_title(f'{model_name} - ROC Curve', fontsize=14, fontweight='bold')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{model_name.lower().replace(\" \", \"_\")}_evaluation.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'confusion_matrix': cm\n",
        "    }\n",
        "\n",
        "# Evaluate all models\n",
        "results = {}\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUATING MODELS ON TEST SET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results['Simple CNN'] = evaluate_model(model1, test_loader, \"Simple CNN\")\n",
        "results['ResNet18'] = evaluate_model(model2, test_loader, \"ResNet18\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all models\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "comparison_df = pd.DataFrame(results).T\n",
        "print(\"\\n\" + comparison_df.to_string())\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
        "for idx, metric in enumerate(metrics):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    comparison_df[metric].plot(kind='bar', ax=ax, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
        "    ax.set_title(f'{metric.upper()} Comparison', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel(metric.upper())\n",
        "    ax.set_xlabel('Model')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_ylim([0, 1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Hyperparameter Tuning with Optuna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective(trial, model_type='resnet18'):\n",
        "    \"\"\"Objective function for Optuna hyperparameter optimization\"\"\"\n",
        "    \n",
        "    # Suggest hyperparameters\n",
        "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
        "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
        "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.3, 0.7)\n",
        "    \n",
        "    # Create model\n",
        "    if model_type == 'resnet18':\n",
        "        model = create_resnet18(num_classes=2, pretrained=True)\n",
        "        # Modify dropout if needed\n",
        "        if hasattr(model, 'fc'):\n",
        "            if isinstance(model.fc, nn.Sequential):\n",
        "                for module in model.fc:\n",
        "                    if isinstance(module, nn.Dropout):\n",
        "                        module.p = dropout_rate\n",
        "    else:\n",
        "        model = SimpleCNN(num_classes=2)\n",
        "    \n",
        "    model = model.to(device)\n",
        "    \n",
        "    # Create data loaders with suggested batch size\n",
        "    train_loader_tune = DataLoader(\n",
        "        train_dataset, \n",
        "        batch_size=batch_size, \n",
        "        shuffle=True,\n",
        "        num_workers=2 if torch.cuda.is_available() else 0\n",
        "    )\n",
        "    \n",
        "    val_loader_tune = DataLoader(\n",
        "        val_dataset, \n",
        "        batch_size=batch_size, \n",
        "        shuffle=False,\n",
        "        num_workers=2 if torch.cuda.is_available() else 0\n",
        "    )\n",
        "    \n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights).to(device) if class_weights else None)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    \n",
        "    # Train for a few epochs (reduced for faster tuning)\n",
        "    num_epochs_tune = 5  # Reduced for faster hyperparameter search\n",
        "    best_val_acc = 0.0\n",
        "    \n",
        "    for epoch in range(num_epochs_tune):\n",
        "        # Train\n",
        "        model.train()\n",
        "        for inputs, labels in train_loader_tune:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        # Validate\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader_tune:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        val_acc = 100 * correct / total\n",
        "        best_val_acc = max(best_val_acc, val_acc)\n",
        "        \n",
        "        # Report intermediate result\n",
        "        trial.report(val_acc, epoch)\n",
        "        \n",
        "        # Handle pruning\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "    \n",
        "    return best_val_acc\n",
        "\n",
        "print(\"✓ Hyperparameter tuning function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run hyperparameter tuning\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HYPERPARAMETER TUNING WITH OPTUNA\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nThis may take a while...\")\n",
        "\n",
        "# Create study\n",
        "study = optuna.create_study(\n",
        "    direction='maximize',\n",
        "    pruner=optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=2)\n",
        ")\n",
        "\n",
        "# Run optimization (reduce n_trials for faster execution)\n",
        "n_trials = 10  # Increase for better results\n",
        "study.optimize(lambda trial: objective(trial, model_type='resnet18'), n_trials=n_trials)\n",
        "\n",
        "# Print best parameters\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BEST HYPERPARAMETERS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Best trial value (Val Accuracy): {study.best_value:.2f}%\")\n",
        "print(f\"\\nBest parameters:\")\n",
        "for key, value in study.best_params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Visualize optimization history\n",
        "try:\n",
        "    fig = plot_optimization_history(study)\n",
        "    fig.show()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Visualize parameter importance\n",
        "try:\n",
        "    fig = plot_param_importances(study)\n",
        "    fig.show()\n",
        "except:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train final model with best hyperparameters\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING FINAL MODEL WITH BEST HYPERPARAMETERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "best_params = study.best_params\n",
        "\n",
        "# Create model with best hyperparameters\n",
        "final_model = create_resnet18(num_classes=2, pretrained=True)\n",
        "\n",
        "# Create data loaders with best batch size\n",
        "train_loader_final = DataLoader(\n",
        "    train_dataset, \n",
        "    batch_size=best_params['batch_size'], \n",
        "    shuffle=True,\n",
        "    num_workers=2 if torch.cuda.is_available() else 0\n",
        ")\n",
        "\n",
        "val_loader_final = DataLoader(\n",
        "    val_dataset, \n",
        "    batch_size=best_params['batch_size'], \n",
        "    shuffle=False,\n",
        "    num_workers=2 if torch.cuda.is_available() else 0\n",
        ")\n",
        "\n",
        "# Train with best hyperparameters\n",
        "final_model, final_history = train_model(\n",
        "    final_model, \n",
        "    train_loader_final, \n",
        "    val_loader_final, \n",
        "    num_epochs=15,  # More epochs for final model\n",
        "    lr=best_params['lr'],\n",
        "    weight_decay=best_params['weight_decay'],\n",
        "    class_weights=class_weights\n",
        ")\n",
        "\n",
        "# Evaluate final model\n",
        "final_results = evaluate_model(final_model, test_loader, \"Final Optimized Model\")\n",
        "\n",
        "# Save final model\n",
        "torch.save(final_model.state_dict(), 'best_deepfake_model.pth')\n",
        "print(\"\\n✓ Final model saved as 'best_deepfake_model.pth'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Summary and Conclusions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PROJECT SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n1. DATASET:\")\n",
        "print(f\"   - Total samples: {len(train_data)}\")\n",
        "print(f\"   - Training samples: {len(train_dataset)}\")\n",
        "print(f\"   - Validation samples: {len(val_dataset)}\")\n",
        "print(f\"   - Test samples: {len(test_dataset)}\")\n",
        "\n",
        "print(\"\\n2. MODELS TRAINED:\")\n",
        "print(\"   - Simple CNN\")\n",
        "print(\"   - ResNet18 (Transfer Learning)\")\n",
        "print(\"   - Optimized ResNet18 (After Hyperparameter Tuning)\")\n",
        "\n",
        "print(\"\\n3. BEST MODEL PERFORMANCE:\")\n",
        "if 'final_results' in locals():\n",
        "    print(f\"   - Accuracy:  {final_results['accuracy']:.4f}\")\n",
        "    print(f\"   - Precision: {final_results['precision']:.4f}\")\n",
        "    print(f\"   - Recall:    {final_results['recall']:.4f}\")\n",
        "    print(f\"   - F1-Score:  {final_results['f1']:.4f}\")\n",
        "    print(f\"   - ROC-AUC:   {final_results['roc_auc']:.4f}\")\n",
        "\n",
        "print(\"\\n4. KEY FINDINGS:\")\n",
        "print(\"   - Transfer learning models (ResNet18) performed better than simple CNN\")\n",
        "print(\"   - Hyperparameter tuning improved model performance\")\n",
        "print(\"   - Feature engineering provided additional insights into data characteristics\")\n",
        "\n",
        "print(\"\\n5. NEXT STEPS:\")\n",
        "print(\"   - Experiment with ensemble methods\")\n",
        "print(\"   - Try more advanced architectures (Vision Transformers)\")\n",
        "print(\"   - Implement temporal features for video data\")\n",
        "print(\"   - Deploy model for real-world inference\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PROJECT COMPLETE!\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
